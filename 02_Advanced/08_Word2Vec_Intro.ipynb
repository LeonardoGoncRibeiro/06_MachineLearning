{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "08_Word2Vec_Intro.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeonardoGoncRibeiro/06_MachineLearning/blob/main/02_Advanced/08_Word2Vec_Intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word embedding using Word2Vec\n",
        "\n",
        "In this course, we will work with word embedding techniques. For that end, we will make a model that classify news. We will start by taking a dataset, and representing it using one-hot-encoding. Then, we will use a different representation using Word2Vec. Word2Vec is able to understand the context of the word. We will discuss different techniques using Word2Vec.\n",
        "\n",
        "In this course, we will use the following packages:"
      ],
      "metadata": {
        "id": "2MRve6nBNS29"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "m2cFxpppL1Ad"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, we will use the following dataset:"
      ],
      "metadata": {
        "id": "A5bfzqjROS6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoXzZ4ReF3Sf",
        "outputId": "50a874db-3381-45f4-d812-fceddd10fa32"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Machine Learning/Avançado/treino.csv')\n",
        "df_test  = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Machine Learning/Avançado/teste.csv')"
      ],
      "metadata": {
        "id": "v_-RFy9IObez"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since our dataset is very large, we are storing them in Google Drive, and importing directly from it. Let's look at our data:"
      ],
      "metadata": {
        "id": "eOQK4V0DP0Yz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head( )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "LzFFCi8CP5GA",
        "outputId": "fa695810-121b-4137-c8d8-d666b1c682c2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               title  \\\n",
              "0  Após polêmica, Marine Le Pen diz que abomina n...   \n",
              "1  Macron e Le Pen vão ao 2º turno na França, em ...   \n",
              "2  Apesar de larga vitória nas legislativas, Macr...   \n",
              "3  Governo antecipa balanço, e Alckmin anuncia qu...   \n",
              "4  Após queda em maio, a atividade econômica sobe...   \n",
              "\n",
              "                                                text        date   category  \\\n",
              "0  A candidata da direita nacionalista à Presidên...  2017-04-28      mundo   \n",
              "1  O centrista independente Emmanuel Macron e a d...  2017-04-23      mundo   \n",
              "2  As eleições legislativas deste domingo (19) na...  2017-06-19      mundo   \n",
              "3  O número de ocorrências de homicídios dolosos ...  2015-07-24  cotidiano   \n",
              "4  A economia cresceu 0,25% no segundo trimestre,...  2017-08-17    mercado   \n",
              "\n",
              "  subcategory                                               link  \n",
              "0         NaN  http://www1.folha.uol.com.br/mundo/2017/04/187...  \n",
              "1         NaN  http://www1.folha.uol.com.br/mundo/2017/04/187...  \n",
              "2         NaN  http://www1.folha.uol.com.br/mundo/2017/06/189...  \n",
              "3         NaN  http://www1.folha.uol.com.br/cotidiano/2015/07...  \n",
              "4         NaN  http://www1.folha.uol.com.br/mercado/2017/08/1...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d5f6d108-1c1e-4cb8-8a19-441bba2be8a0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>date</th>\n",
              "      <th>category</th>\n",
              "      <th>subcategory</th>\n",
              "      <th>link</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Após polêmica, Marine Le Pen diz que abomina n...</td>\n",
              "      <td>A candidata da direita nacionalista à Presidên...</td>\n",
              "      <td>2017-04-28</td>\n",
              "      <td>mundo</td>\n",
              "      <td>NaN</td>\n",
              "      <td>http://www1.folha.uol.com.br/mundo/2017/04/187...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Macron e Le Pen vão ao 2º turno na França, em ...</td>\n",
              "      <td>O centrista independente Emmanuel Macron e a d...</td>\n",
              "      <td>2017-04-23</td>\n",
              "      <td>mundo</td>\n",
              "      <td>NaN</td>\n",
              "      <td>http://www1.folha.uol.com.br/mundo/2017/04/187...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Apesar de larga vitória nas legislativas, Macr...</td>\n",
              "      <td>As eleições legislativas deste domingo (19) na...</td>\n",
              "      <td>2017-06-19</td>\n",
              "      <td>mundo</td>\n",
              "      <td>NaN</td>\n",
              "      <td>http://www1.folha.uol.com.br/mundo/2017/06/189...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Governo antecipa balanço, e Alckmin anuncia qu...</td>\n",
              "      <td>O número de ocorrências de homicídios dolosos ...</td>\n",
              "      <td>2015-07-24</td>\n",
              "      <td>cotidiano</td>\n",
              "      <td>NaN</td>\n",
              "      <td>http://www1.folha.uol.com.br/cotidiano/2015/07...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Após queda em maio, a atividade econômica sobe...</td>\n",
              "      <td>A economia cresceu 0,25% no segundo trimestre,...</td>\n",
              "      <td>2017-08-17</td>\n",
              "      <td>mercado</td>\n",
              "      <td>NaN</td>\n",
              "      <td>http://www1.folha.uol.com.br/mercado/2017/08/1...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d5f6d108-1c1e-4cb8-8a19-441bba2be8a0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d5f6d108-1c1e-4cb8-8a19-441bba2be8a0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d5f6d108-1c1e-4cb8-8a19-441bba2be8a0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our dataset stores different news. So, we have a title, a text, a date, and a category. Also, we have the link for our news. Let's see how many data do we have:"
      ],
      "metadata": {
        "id": "BSk8W1jQQaab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiAWgBcAQZMO",
        "outputId": "00c59548-2561-4357-ec36-c753144603fd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(90000, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NLI6Xt9Qldq",
        "outputId": "5eabf0d2-e444-406b-c13f-4bcf29be6b68"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20513, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our training set, we have 90000 news, and, in our test set, we have 20513 news.\n",
        "\n",
        "In this course, we will create a model that receives our news, and predicts the category. However, we can't fit a model to our text itself. First, we have to create a representation of the text that the model can understand. The most simple representation is the one-hot encoding."
      ],
      "metadata": {
        "id": "O-X8PeX0QnAK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One-hot encoding\n",
        "\n",
        "Using One-hot encoding, we are considering that our each word in our text is a feature, and then we use a binary encoding to state whether our text has the word or not. We can do that using ```CountVectorizer( )```. For instance:"
      ],
      "metadata": {
        "id": "7yVPaimRQ4Fb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\"Have a nice day\", \"Have a bad day\", 'Have a great day']\n",
        "\n",
        "vec   = CountVectorizer( )\n",
        "vec.fit(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91cqdbVdQmQY",
        "outputId": "2eee45e0-de8a-42b8-e84b-27f5cffce7ad"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer()"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's look at the vector for each phrase:"
      ],
      "metadata": {
        "id": "sM9_zx9ZTDH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_nice = vec.transform([\"Have a nice day\"])\n",
        "print(vector_nice.toarray( ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79uFuxmzS93W",
        "outputId": "675a8f4c-f244-4b68-b0a8-838dd054f861"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 1 0 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that we transformed our text into an array representation. Note that, here, our vector has 5 positions. That occurs because we have 5 words in our vocabulary. \n",
        "\n",
        "The one-hot encoding representation is simple and interesting. However, if we have a big vocabulary (which is expected in our case, working with more than 100,000 news), we will have too many features. In these cases, other representations may become much more interesting, such as..."
      ],
      "metadata": {
        "id": "9mtSqNsLTXLi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word2Vec\n",
        "\n",
        "Using Word2Vec, we also make a vectorized representation for our text data. However, Word2Vec is known as a word embedding technique, which uses dense vectors to store the text. This makes it so that less memory is required to store our data.\n",
        "\n",
        "Using Word2Vec, we end up taking similar words, that are used in similar contexts, and make them \"closer\" in the vector. To make this transformation, we need to use an already fitted model, that already knows the relation between such words.\n",
        "\n",
        "The training of an embedding is similar to the training of a Neural Network: we train them to get the optimal weight values in a vector of numbers (which better represent each text). We can train our Word2Vec model using a continuous bag of words (CBOW), where we pass a context and try to fit each word, seeing if they fit there, such as:\n",
        "\n",
        "I live in \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n",
        "\n",
        "We can also do the opposite, where we send a word and try to identify the context. This technique is known as Skip Gram. Both techniques can be used to see which one draws better results for our data.\n",
        "\n",
        "In this course, we will not train an word embedding technique, but rather we will use an already fitted model. This model is taken from:\n",
        "\n",
        "http://www.nilc.icmc.usp.br/embeddings\n",
        "\n",
        "So, let's unzip our model:"
      ],
      "metadata": {
        "id": "pdfP9UOSUBSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/drive/MyDrive/Colab Notebooks/Machine Learning/Avançado/cbow_s300.zip\""
      ],
      "metadata": {
        "id": "qS4YtJ3BUA2F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccb76491-63d6-45dc-80af-a41d53cbfbdb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/Colab Notebooks/Machine Learning/Avançado/cbow_s300.zip\n",
            "  inflating: cbow_s300.txt           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The file that we have is a text file containing all words and their corresponding vectors. To load the Word2Vec model, we can do:"
      ],
      "metadata": {
        "id": "3MeTjRgGDh2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = KeyedVectors.load_word2vec_format(\"cbow_s300.txt\")"
      ],
      "metadata": {
        "id": "VNQg9EFzD_2F"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nice! Finally, we can obtain the vector for a given word. For instance, let's get the vector for *china*:"
      ],
      "metadata": {
        "id": "V-M4gkmYFEe3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_vector(\"china\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eI8Bu75FLjs",
        "outputId": "2c40f143-3c40-411d-abfa-51e7c60a8c85"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1.49033e-01,  1.26020e-01,  2.17628e-01,  1.82684e-01,\n",
              "        1.65151e-01, -1.59660e-01, -2.34411e-01,  6.00570e-02,\n",
              "        8.03680e-02,  2.87578e-01, -4.81100e-03, -5.68800e-02,\n",
              "        2.15676e-01,  8.65540e-02,  1.25983e-01,  3.36157e-01,\n",
              "       -1.83254e-01, -1.18499e-01,  1.13010e-02,  1.03814e-01,\n",
              "        9.37640e-02,  2.90178e-01, -1.64395e-01, -1.13300e-02,\n",
              "       -1.80676e-01, -1.15820e-02,  1.08728e-01,  1.65898e-01,\n",
              "        9.37900e-02,  2.66767e-01, -1.29890e-02,  9.16030e-02,\n",
              "        2.21292e-01, -1.36497e-01, -4.26350e-02, -1.30038e-01,\n",
              "        2.17067e-01, -1.01963e-01, -3.70960e-02,  1.42155e-01,\n",
              "        3.41109e-01,  2.46560e-01,  1.27458e-01,  5.72360e-02,\n",
              "       -1.47962e-01, -1.60290e-02,  1.86533e-01,  7.71550e-02,\n",
              "       -3.50024e-01, -4.06085e-01,  1.67131e-01, -4.75230e-02,\n",
              "        5.13780e-02, -1.28224e-01,  1.06580e-02, -2.92652e-01,\n",
              "        1.40540e-01, -4.57049e-01,  1.31094e-01,  2.03234e-01,\n",
              "        2.94019e-01,  7.38370e-02,  1.11554e-01, -1.64204e-01,\n",
              "       -3.62020e-02,  1.29522e-01, -1.28321e-01,  1.37502e-01,\n",
              "       -7.99200e-03, -5.07100e-03, -2.86010e-02, -8.99040e-02,\n",
              "        8.82800e-03, -8.27730e-02,  6.91940e-02, -2.70182e-01,\n",
              "        5.47610e-02, -3.06060e-02,  6.89880e-02,  2.38759e-01,\n",
              "       -1.41775e-01,  2.34763e-01, -2.23853e-01, -2.84994e-01,\n",
              "        2.53245e-01,  6.77170e-02, -4.39663e-01, -7.00270e-02,\n",
              "        6.39150e-02, -9.67100e-02, -2.18950e-01, -5.77910e-02,\n",
              "       -1.82689e-01, -3.32202e-01, -7.83070e-02,  7.74620e-02,\n",
              "        8.82920e-02, -4.83618e-01, -1.77812e-01,  5.64040e-02,\n",
              "        1.50339e-01,  8.73000e-02, -1.03121e-01,  1.62065e-01,\n",
              "        4.57940e-02,  9.73590e-02,  1.67230e-02,  3.00791e-01,\n",
              "       -6.49640e-02, -1.95840e-01, -4.33790e-02, -9.46810e-02,\n",
              "        3.73222e-01, -1.65359e-01,  5.58780e-02,  1.72660e-02,\n",
              "       -3.16048e-01,  9.24430e-02, -6.84540e-02, -3.57085e-01,\n",
              "       -1.69469e-01, -1.14090e-01,  9.47230e-02,  3.14999e-01,\n",
              "        2.12717e-01, -2.21540e-02,  1.76870e-02,  1.58473e-01,\n",
              "       -1.39150e-02,  1.23610e-02, -4.13190e-02, -1.47159e-01,\n",
              "       -1.00070e-02,  3.41884e-01,  1.16999e-01, -5.01590e-02,\n",
              "        7.88740e-02,  6.27940e-02,  2.73643e-01,  1.46823e-01,\n",
              "       -1.68857e-01, -1.00014e-01, -5.41060e-02, -3.06130e-02,\n",
              "       -8.85920e-02, -6.19840e-02,  1.21595e-01,  1.13775e-01,\n",
              "        3.97190e-02, -8.54000e-03,  1.05670e-02,  1.12375e-01,\n",
              "        9.70000e-02,  9.05850e-02,  1.25026e-01, -2.92209e-01,\n",
              "        6.81330e-02,  4.06070e-02,  1.33042e-01, -9.77780e-02,\n",
              "       -3.26378e-01,  9.71420e-02, -5.13600e-02,  2.01450e-02,\n",
              "        1.20182e-01, -2.14210e-02, -1.30884e-01,  9.52800e-02,\n",
              "       -5.65320e-02, -8.35370e-02, -2.53035e-01,  9.18650e-02,\n",
              "        7.89190e-02, -6.30710e-02, -1.64057e-01,  8.31660e-02,\n",
              "        1.42698e-01, -2.77053e-01,  7.05810e-02, -1.37800e-02,\n",
              "       -2.74883e-01,  3.02011e-01, -8.34330e-02, -1.14381e-01,\n",
              "       -2.88826e-01,  9.03960e-02,  1.94704e-01, -1.57261e-01,\n",
              "       -2.58910e-02,  1.41321e-01, -1.67231e-01, -2.91540e-02,\n",
              "        8.03650e-02,  1.27378e-01, -1.48120e-01,  2.83291e-01,\n",
              "       -2.65930e-02,  2.15319e-01,  3.35030e-02,  6.47140e-02,\n",
              "        4.20010e-02, -3.85537e-01, -2.67068e-01, -2.77017e-01,\n",
              "       -1.82289e-01, -1.18735e-01, -2.51480e-01, -1.83783e-01,\n",
              "       -2.12362e-01,  2.50214e-01,  3.96240e-02,  2.64830e-02,\n",
              "        1.30810e-01, -1.38478e-01, -1.63040e-02, -2.55850e-02,\n",
              "        2.35141e-01, -8.80540e-02, -9.40650e-02,  1.31790e-01,\n",
              "       -8.33330e-02, -2.40020e-02, -3.38183e-01,  8.10370e-02,\n",
              "       -1.68933e-01,  1.92200e-03,  9.34870e-02,  6.58130e-02,\n",
              "       -1.11925e-01,  1.83907e-01, -6.54900e-03,  4.27730e-02,\n",
              "        3.71566e-01, -3.08570e-02,  1.99647e-01,  1.25516e-01,\n",
              "        1.38471e-01, -9.17400e-02, -2.27814e-01,  8.27690e-02,\n",
              "       -2.94581e-01,  9.56830e-02, -3.48070e-01,  1.02342e-01,\n",
              "       -8.05350e-02,  2.34290e-02, -4.19860e-02,  2.44763e-01,\n",
              "        2.37160e-02, -2.23548e-01, -9.26800e-03, -4.33650e-02,\n",
              "       -1.12413e-01, -4.19178e-01,  1.81267e-01,  1.03648e-01,\n",
              "        2.74945e-01, -9.23560e-02, -4.63300e-02, -2.06314e-01,\n",
              "        4.81410e-02,  2.64603e-01, -1.17113e-01,  1.80097e-01,\n",
              "        5.54220e-02, -1.27460e-01, -1.60328e-01,  1.02289e-01,\n",
              "        4.09530e-02,  1.25305e-01,  1.53398e-01, -2.36950e-02,\n",
              "        2.33967e-01,  2.30250e-02, -1.40227e-01,  3.16349e-01,\n",
              "       -1.99592e-01,  1.25398e-01,  2.72858e-01,  1.09793e-01,\n",
              "       -1.64379e-01,  8.63630e-02,  1.97445e-01, -2.18180e-02,\n",
              "       -1.49784e-01, -3.34461e-01, -4.61000e-04,  1.92640e-02,\n",
              "        2.11149e-01, -2.93349e-01,  5.90160e-02,  1.25044e-01,\n",
              "        7.75570e-02, -2.82863e-01, -3.38890e-02, -9.19950e-02,\n",
              "       -1.43850e-01,  1.45775e-01,  1.04246e-01, -2.60548e-01],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we installed the model with 300 dimensions, this vector has 300 positions:"
      ],
      "metadata": {
        "id": "WpvQg7aBFTPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(model.get_vector(\"china\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpzYC8dtFOTR",
        "outputId": "25d9789d-cf1d-4da9-9515-46ee2ab4261e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2Vec representation can also help us to search for the most similar words:"
      ],
      "metadata": {
        "id": "R9KZu8Z1FbOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.most_similar(\"china\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZ4HGiChFfUL",
        "outputId": "f9af1f9f-f0fa-4010-a142-89be4e4d8bb8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('rússia', 0.7320704460144043),\n",
              " ('índia', 0.7241617441177368),\n",
              " ('tailândia', 0.701935887336731),\n",
              " ('indonésia', 0.6860769987106323),\n",
              " ('turquia', 0.6741335988044739),\n",
              " ('malásia', 0.6665689945220947),\n",
              " ('mongólia', 0.6593616008758545),\n",
              " ('manchúria', 0.6581847667694092),\n",
              " ('urss', 0.6581669449806213),\n",
              " ('grã-bretanha', 0.6568098068237305)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nice! Note that the most similar words are always asian countries, which are actually close to China! "
      ],
      "metadata": {
        "id": "dUabfQNxFh8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding Word2Vec vectors\n",
        "\n",
        "Note that, now, we have multiple 300-dimensional arrays that represent each word. To understand which words are similar to others, we are essentially evaluating the distance between two vectors. We can also take the most similar words considering multiple vectors:"
      ],
      "metadata": {
        "id": "r3JPouXOGYlv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.most_similar(positive = ['brasil', 'argentina'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvPBw2v1Fr4i",
        "outputId": "8b53c1e3-bca0-4fb5-8893-c0e472992233"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('chile', 0.6781662702560425),\n",
              " ('peru', 0.6348033547401428),\n",
              " ('venezuela', 0.6273865103721619),\n",
              " ('equador', 0.6037014722824097),\n",
              " ('bolívia', 0.6017140746116638),\n",
              " ('haiti', 0.5993807315826416),\n",
              " ('méxico', 0.5962306261062622),\n",
              " ('paraguai', 0.5957703590393066),\n",
              " ('uruguai', 0.5903672575950623),\n",
              " ('japão', 0.5893509984016418)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another interesting thing we can do with Word2Vec is trying to get the plural of a given word. We can try to do that using:"
      ],
      "metadata": {
        "id": "cEE4QNAaIlK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.most_similar(positive = [\"nuvens\", \"estrela\"], negative = [\"nuvem\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vec_36VOIgcz",
        "outputId": "5a301326-766c-4aca-e5a1-e2b8837819cd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('estrelas', 0.5497429966926575),\n",
              " ('plêiades', 0.379197895526886),\n",
              " ('colinas', 0.3746805191040039),\n",
              " ('trovoadas', 0.373703271150589),\n",
              " ('sombras', 0.3734194040298462),\n",
              " ('pombas', 0.3726757764816284),\n",
              " ('corredoras', 0.3640727698802948),\n",
              " ('cigarras', 0.36065393686294556),\n",
              " ('galáxias', 0.35754913091659546),\n",
              " ('luas', 0.3575345277786255)]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nice! Note that here we got a singular and plural word (nuvem and nuvens) and used them to get the plural of another (estrelas). Let's try to use a similar idea to get other derivations:"
      ],
      "metadata": {
        "id": "1IG0KoFXJH6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.most_similar(positive = [\"professor\", \"mulher\"], negative = [\"homem\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcLYJhHcJETH",
        "outputId": "50482ac9-5cd8-4ae8-a725-9bf9fc6aa8d9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('professora', 0.6192208528518677),\n",
              " ('aluna', 0.5449554324150085),\n",
              " ('esposa', 0.4978231191635132),\n",
              " ('ex-aluna', 0.4884248375892639),\n",
              " ('namorada', 0.4737858772277832),\n",
              " ('enfermeira', 0.4728144109249115),\n",
              " ('filha', 0.467373788356781),\n",
              " ('irmã', 0.45845913887023926),\n",
              " ('ex-namorada', 0.45824766159057617),\n",
              " ('ex-professora', 0.4510470926761627)]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vectorization of our data\n",
        "\n",
        "Ok, so, we have imported our Word2Vec model. Now, let's perform the vectorization of our data. Here, we will use the titles. For instance, let's get one of our titles:"
      ],
      "metadata": {
        "id": "asgyTEuIQegc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.title.iloc[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "77aGWk1VJaHy",
        "outputId": "7d6094b3-81dd-4a80-ba5b-830e7f76cfc5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Após polêmica, Marine Le Pen diz que abomina negacionistas do Holocausto'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our title, we have many words. So, first, we have to tokenize our text, so that, later, we can use word embeddings to get the vectors for each word.\n",
        "\n",
        "Let's crete a function to tokenize our text:"
      ],
      "metadata": {
        "id": "3CobW_DyQ1SO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DKUON5hR-kp",
        "outputId": "18d02dbc-5882-48d2-c095-0d859a19a515"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def GetTokens(text):\n",
        "  alphanumeric_list = []\n",
        "\n",
        "  text_lower = text.lower( )\n",
        "\n",
        "  tokens = nltk.word_tokenize(text_lower)\n",
        "\n",
        "  for token in tokens:\n",
        "    if token not in string.punctuation:\n",
        "      alphanumeric_list.append(token)\n",
        "\n",
        "  return alphanumeric_list"
      ],
      "metadata": {
        "id": "tc1ESbenQuBp"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nice! Now, let's test our function:"
      ],
      "metadata": {
        "id": "2CUj2IV3R5l_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_example = GetTokens(df_train.title.iloc[0])\n",
        "tokens_example"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rK3NVsAhRVwg",
        "outputId": "02485385-3c9c-43ce-823a-236d9e34a35d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['após',\n",
              " 'polêmica',\n",
              " 'marine',\n",
              " 'le',\n",
              " 'pen',\n",
              " 'diz',\n",
              " 'que',\n",
              " 'abomina',\n",
              " 'negacionistas',\n",
              " 'do',\n",
              " 'holocausto']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It worked!"
      ],
      "metadata": {
        "id": "4qFgLGTaSDDA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combining our words to get the resulting vector\n",
        "\n",
        "Our Word2Vec model gives us different vectors for different words. Now, we need to get the resulting vector for the text. There are multiple methods to perform this (https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/71778). Here, we will use the most simple of them, which simply gets the vector combination by summing them up:"
      ],
      "metadata": {
        "id": "o6zYt8aQSKjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def SumCombination(tokens):\n",
        "  resulting_vector = np.zeros(300)               # Initializing our vector\n",
        "  \n",
        "  for token in tokens:\n",
        "    resulting_vector += model.get_vector(token)\n",
        "\n",
        "  return resulting_vector"
      ],
      "metadata": {
        "id": "tzbvIg2XR9qD"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nice! Now, let's test it:"
      ],
      "metadata": {
        "id": "6wIat1lKTLe_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vec_example = SumCombination(tokens_example)\n",
        "vec_example"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbpYDkuxTHge",
        "outputId": "b6d9de6f-8ac7-4f15-8376-9cc1c39341b4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.52357099,  1.00529201,  0.26381801,  0.55167001,  1.29887401,\n",
              "       -0.300037  ,  0.89378801,  0.11975599, -1.45775002, -0.452488  ,\n",
              "        0.458223  , -0.21807001,  0.65952699,  0.57539101,  0.57576102,\n",
              "        0.05649301, -1.28612098,  0.74008099,  0.26500801, -0.77916699,\n",
              "       -1.012431  ,  0.140885  , -0.555113  ,  0.71819499, -0.20966199,\n",
              "        0.64139499, -0.96246603,  0.13343704,  0.38299598,  0.83808199,\n",
              "        0.12288502, -1.115068  , -0.00647301, -0.57349899, -0.78776202,\n",
              "       -0.24487801, -0.10404397, -1.67126699, -0.08068499, -0.51521898,\n",
              "        0.04381603,  0.68886297,  1.045672  , -0.23395702, -0.62220401,\n",
              "        0.98942098,  0.43884596,  1.37832599, -0.183351  , -0.36253401,\n",
              "        0.35007997, -0.75641701, -1.36119601,  0.143518  ,  0.00893804,\n",
              "        0.49978199,  0.069786  , -0.513827  , -0.49270102,  0.86613399,\n",
              "       -0.27306198,  0.228851  ,  0.12569697, -0.23421999, -0.20668898,\n",
              "       -0.07144199, -1.13684901,  0.74301703, -0.15537495,  0.16737801,\n",
              "       -0.26599302, -0.32244399, -0.4586    , -0.55695101, -0.40938401,\n",
              "       -0.81748699, -0.66862201, -0.07216502, -0.44141797,  0.24551399,\n",
              "        0.068979  , -0.73147901, -0.38973701,  0.45652699, -0.85513303,\n",
              "        0.584598  , -0.87265899, -0.32530599, -0.61205298,  0.82195199,\n",
              "       -0.55980601,  0.149703  , -0.126987  ,  0.48916601, -0.687582  ,\n",
              "       -0.132848  , -0.32872301, -1.09678699,  0.509032  , -0.58177099,\n",
              "        1.55592299,  0.09057198,  0.80985001, -0.256162  ,  1.04478799,\n",
              "        0.23986501, -0.702458  ,  0.27091401, -1.21671701, -0.885602  ,\n",
              "       -0.24228   , -0.15859299,  0.097735  , -0.05279702, -0.44079498,\n",
              "        0.138917  ,  0.14013299,  0.25265801,  0.70067604,  0.012048  ,\n",
              "        0.33653299, -0.95202299, -0.64074602,  0.48337203, -0.19100299,\n",
              "        0.367282  , -0.21992799,  0.11430101,  0.58179298,  1.13985902,\n",
              "       -0.02254602,  0.928958  , -0.17897399, -0.51686199, -0.01986698,\n",
              "       -0.45679001, -0.67135799, -0.15633301,  0.86553201,  0.42948302,\n",
              "       -0.280657  , -0.010624  , -0.199618  , -0.260354  , -0.15055199,\n",
              "       -0.20828902, -0.50230499, -0.19077102,  1.151954  , -0.72274601,\n",
              "        0.48594799,  0.40863903, -0.708719  , -0.44239602,  0.23992399,\n",
              "       -0.33462703, -0.04561601, -0.07753295,  0.68536701,  0.29241399,\n",
              "       -0.44281401, -0.17286999, -0.228812  ,  0.25649601,  0.75337702,\n",
              "       -0.493812  ,  0.10178503, -1.61696599,  0.42017299, -0.47151702,\n",
              "        0.395183  , -0.507418  , -0.71153797,  0.27695299, -0.442187  ,\n",
              "        0.13449999,  0.065592  , -0.17892001,  0.18020201, -0.55684003,\n",
              "        0.37161202, -0.22614699, -0.66433799,  0.469644  , -0.74038799,\n",
              "       -0.68302301,  0.12273101, -0.155346  , -0.120688  , -0.135375  ,\n",
              "       -0.25607999,  0.01113099, -0.12008901,  0.48782299, -0.446102  ,\n",
              "       -0.06364999, -0.400148  , -0.227375  ,  0.57061599, -1.60692397,\n",
              "        0.66040799, -0.281954  , -0.04787599, -0.13420899, -0.45828996,\n",
              "        0.56910301, -0.21898799, -0.05955301,  0.044383  ,  0.21575798,\n",
              "       -0.46714798,  0.408052  ,  0.64580799, -0.812993  , -0.32361801,\n",
              "        0.40792201,  2.326033  ,  0.80411299,  0.53480298,  0.11107701,\n",
              "        0.18698001, -0.10990801, -0.04847499,  0.36679599,  0.65713703,\n",
              "        0.79721   ,  0.093056  , -0.54069601,  0.36470501,  0.03096598,\n",
              "        0.44370001,  0.15469401, -0.52987101,  0.06958801,  0.55563197,\n",
              "       -0.35508001, -0.92907302, -0.43656399, -0.04584399,  0.38642502,\n",
              "        0.856273  , -0.82691399, -0.158832  ,  0.34742501,  0.04472601,\n",
              "       -0.62290598, -0.722268  , -0.409217  ,  0.19961499,  0.71964902,\n",
              "       -0.091035  , -0.70253399, -0.06629701,  0.26154   , -0.22917899,\n",
              "       -0.821531  , -0.16808201,  0.21580996, -0.43994202,  0.170879  ,\n",
              "       -0.69552499,  0.38967901,  0.88689099,  0.49874901,  0.26643402,\n",
              "       -0.241595  ,  0.701123  ,  0.50368899,  0.20559299,  0.57624799,\n",
              "        0.23957698, -0.44731001,  0.245827  , -1.35982903,  0.80485501,\n",
              "       -0.010276  ,  0.06499797,  0.49871599, -0.277177  ,  0.032921  ,\n",
              "       -0.092207  , -0.32241401, -1.36030599,  0.29398001,  0.04733601,\n",
              "        0.24439599, -0.506858  , -0.58694102, -0.57482699,  0.22564799,\n",
              "        0.460606  , -0.80701201, -0.65453104, -1.21641301, -0.68294701,\n",
              "       -0.32719002,  0.207949  ,  0.43726099,  0.187696  ,  0.15216099])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems to have worked out. \n",
        "\n",
        "Note that our function will generate an error if we pass a word that is not in its vocabulary. Also, all numeric values were normalized to 0. These can be found in the original paper for the model:\n",
        "\n",
        "https://arxiv.org/abs/1708.06025\n",
        "\n",
        "To fix these, we can raise an exception:"
      ],
      "metadata": {
        "id": "r9deOs7mTTI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def SumCombination(tokens):\n",
        "  resulting_vector = np.zeros(300)               # Initializing our vector\n",
        "  \n",
        "  for token in tokens:\n",
        "    try:\n",
        "      resulting_vector += model.get_vector(token)\n",
        "    except KeyError:\n",
        "      if token.isnumeric( ):\n",
        "        token = \"0\"*len(token)                   # Treating numeric inputs\n",
        "      else:\n",
        "        token = \"unknown\"                        # Treating low frequency words\n",
        "      resulting_vector += model.get_vector(token)\n",
        "\n",
        "  return resulting_vector"
      ],
      "metadata": {
        "id": "QlLMw5_WTq05"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nice! Now, we have treated the possible errors in our code."
      ],
      "metadata": {
        "id": "UZMQruGmWAdC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using our new representation for classification\n",
        "\n",
        "Finally, we can use the new representation of our text for classification purposes. First, let's obtain the vectors corresponding to all titles:"
      ],
      "metadata": {
        "id": "R6936YEpMFMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def GetAllResVectors(texts):\n",
        "  x = len(texts)\n",
        "  y = 300\n",
        "  resulting_vectors = np.zeros((x, y))\n",
        "\n",
        "  for i in range(x):\n",
        "    tokens = GetTokens(texts.iloc[i])\n",
        "    vector = SumCombination(tokens)\n",
        "    resulting_vectors[i] = vector\n",
        "\n",
        "  return resulting_vectors"
      ],
      "metadata": {
        "id": "NR8uzyswUnqm"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = GetAllResVectors(df_train.title)\n",
        "X_test  = GetAllResVectors(df_test.title)"
      ],
      "metadata": {
        "id": "qurhqj2dMxZx"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = df_train.category\n",
        "y_test  = df_test.category"
      ],
      "metadata": {
        "id": "dJzkFKCSOnGG"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, if we get the size of our training and test datasets:"
      ],
      "metadata": {
        "id": "6lUrBuJKOOMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5R1xqaGSN-fz",
        "outputId": "b0f4246b-2ae1-4644-e47c-0016de13353a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(90000, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWrvq3d7OR6W",
        "outputId": "c28c3d86-0b63-4740-a7b0-3609565373cb"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20513, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have 300 features in each dataset. Now, we can fit our model using:"
      ],
      "metadata": {
        "id": "AFoUpGsKOTlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logreg = LogisticRegression(max_iter = 200)\n",
        "logreg.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0X70_3BOSxG",
        "outputId": "36b2bffa-d8fe-4da7-cfa2-a1f322168d0e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(max_iter=200)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check the test accuracy, we can do:"
      ],
      "metadata": {
        "id": "e2OIiV6uPXQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = logreg.score(X_test, y_test)*100\n",
        "print(\"Accuracy: {:.2f}%\".format(acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euf_0URAOyIr",
        "outputId": "ea562d5f-b94c-42dd-d30e-e75d725ad416"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 79.58%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nice! We have found an accuracy of almost 80% in our first model. However, we have six different categories. Let's understand how did our model perform for each of our labels. For that end, we can use a classification report:"
      ],
      "metadata": {
        "id": "kDw901DyPmr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = logreg.predict(X_test)\n",
        "cr = classification_report(y_test, y_pred)\n",
        "print(cr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kViM2Pq-PgKf",
        "outputId": "0a872df6-4911-4da9-920e-3f52e56b6195"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     colunas       0.86      0.71      0.78      6103\n",
            "   cotidiano       0.61      0.79      0.69      1698\n",
            "     esporte       0.92      0.88      0.90      4663\n",
            "   ilustrada       0.13      0.88      0.23       131\n",
            "     mercado       0.84      0.79      0.81      5867\n",
            "       mundo       0.74      0.86      0.79      2051\n",
            "\n",
            "    accuracy                           0.80     20513\n",
            "   macro avg       0.68      0.82      0.70     20513\n",
            "weighted avg       0.83      0.80      0.81     20513\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nice! Now, we have the accuracy (80%) and other metrics, such as precision and recall. Precision says that, from what I am classifying as $x$, only $p$% is really $x$. Recall, on the other hand, says that from class $x$, we are making the right call in $p$% of the guesses. Note that, in most cases, we got a high recall. However, in some of those (such as 'ilustrada') we got a very low precision. That is likely because we have very few samples from this class in our test set. Note that the weighted averages (which take into consideration the number of samples in each class) are very good."
      ],
      "metadata": {
        "id": "vVFKQt2IRVd6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using a dummy classifier\n",
        "\n",
        "To say that our Logistic Regression model showed a good accuracy, we should actually try to fit a baseline model to our data. Our Logistic Regression should be able to show a much better performance when compared to this very simple model. Here, we will use a dummy classifier."
      ],
      "metadata": {
        "id": "lhyFhazBTS2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dummy = DummyClassifier( )\n",
        "dummy.fit(X_train, y_train)\n",
        "acc = dummy.score(X_test, y_test)*100\n",
        "print(\"Accuracy: {:.2f}%\".format(acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-k2YYdJR9VF",
        "outputId": "b46db123-c31e-4c6e-97ce-6454ae5d5acb"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 29.75%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our dummy classifier actually showed a very poor accuracy. Let's get the classification report:"
      ],
      "metadata": {
        "id": "wjzF9Gf7Tv6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = dummy.predict(X_test)\n",
        "cr = classification_report(y_test, y_pred)\n",
        "print(cr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6TFcWMzTt6z",
        "outputId": "f0d14ae2-efcf-48e1-e494-a920069a93d8"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     colunas       0.30      1.00      0.46      6103\n",
            "   cotidiano       0.00      0.00      0.00      1698\n",
            "     esporte       0.00      0.00      0.00      4663\n",
            "   ilustrada       0.00      0.00      0.00       131\n",
            "     mercado       0.00      0.00      0.00      5867\n",
            "       mundo       0.00      0.00      0.00      2051\n",
            "\n",
            "    accuracy                           0.30     20513\n",
            "   macro avg       0.05      0.17      0.08     20513\n",
            "weighted avg       0.09      0.30      0.14     20513\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that, indeed, our Logistic Regression model is much better than the Dummy Classifier model. This shows that our model is really being able to understand the patterns in the data, helping us to perform the classification of our news."
      ],
      "metadata": {
        "id": "1zql1dmLT5f7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extra: What could we have done differently?\n",
        "\n",
        "There are many things that we could have done to try and improve our model prediction. First, we could try to use a CBOW model with even more dimensions. On NILC's website, we can find CBOW models with up to 1000 dimensions. Also, we could try to use SKIPGRAM models and check if the resulting accuracy is higher. Another thing we could try is to change how we are combining our vectors for the text (here, we simply sum the vectors). Finally, we could try to use different, more complex models to perform our final classification."
      ],
      "metadata": {
        "id": "BECgvWfFUkPi"
      }
    }
  ]
}