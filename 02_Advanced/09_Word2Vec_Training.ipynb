{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeonardoGoncRibeiro/06_MachineLearning/blob/main/02_Advanced/09_Word2Vec_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sw-HMOzXYGGY"
      },
      "source": [
        "# Word2Vec: Training a word embedding\n",
        "\n",
        "In this course, we will show how can we train a word embedding model. First, we will see how to use Spacy to preprocess textual data. Then, we will understand how to set hyperparameters for the Word2Vec model. The fitting of our model can be performed using Gensim. In the end, we will create a text classifier using our Word2Vec model, and we will learn how to save a file for our model to use it later.\n",
        "\n",
        "In this course, we will use the following packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tXKZkB3eLEc",
        "outputId": "a44cacf0-8840-4316-c620-d3fac4e80977"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-08-03 20:09:27.128494: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "\u001b[K     |████████████████████████████████| 13.0 MB 344 kB/s \n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download pt_core_news_sm --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "GZd0BbcxYBK_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "import logging\n",
        "\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVjOjTMvYoIj"
      },
      "source": [
        "Also, we will use the following dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "i8ND17xFYnk7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0152a6be-1316-4834-ba8b-eab6bf7cb876"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "J7h_bMG1dYTS"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Machine Learning/Avançado/treino.csv')\n",
        "df_test  = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Machine Learning/Avançado/teste.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSsKAICgdOpr"
      },
      "source": [
        "# Spacy\n",
        "\n",
        "Spacy is a very strong open-source library for NLP. Here, we will use Spacy to preprocess our data. Since we will use a model for the portuguese language, let's import our portuguese data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "hVrPjDddei_F"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('pt_core_news_sm', disable = [\"parser\", \"ner\", \"tagger\", \"textcat\"])   # We disabled some features to improve efficiency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHBJ3oQle6Qj"
      },
      "source": [
        "This object can be used to transform our text into ```doc``` typing. For instance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DR65bn2sele_",
        "outputId": "3f948387-8218-4ed6-a843-aaa861fff312"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.tokens.doc.Doc"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "text = \"Rio de Janeiro é uma cidade maravilhosa.\"\n",
        "doc = nlp(text)\n",
        "type(doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4ora8SKfPwC"
      },
      "source": [
        "This typing stores different tokens for our data. For instance, we can get the first word in our text using:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4sChcD6fMaJ",
        "outputId": "5703f6f3-6952-4f7b-be44-9db2ee880c10"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Rio"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "doc[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBAOMTYGfa9D"
      },
      "source": [
        "Nice! Also, we can check the entities in our text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7U1NF1S4fZ9_",
        "outputId": "87fc7c99-e17a-490b-80dc-82184eb66c6e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Rio de Janeiro,)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "doc.ents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vog_qJMKf7QQ"
      },
      "source": [
        "We can also check if something in our text is a stopword. For instance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPPUJ_IefcDk",
        "outputId": "b3c695f1-840d-48ad-c3b3-75fcd474f51d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "doc[0].is_stop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maOh0OMJgCEd"
      },
      "source": [
        "So, it seems that the first word in our text is not a stopword.\n",
        "\n",
        "Note that, in this course, we will not use One-hot encoding representation. Instead, we will used Word2Vec. We will take many words from our corpus, and try to define optimal vector representations with a fixed size. The definition of the optimal vectors is similar to the training of a Neural Network.\n",
        "\n",
        "To train a Word2Vec model, we should start by performing a preprocessing over our text. Thus, we should do:\n",
        "\n",
        "* Put our text in lower case.\n",
        "* Exclude punctuation.\n",
        "* Exclude stopwords.\n",
        "* Get titles with, at least, three words.\n",
        "\n",
        "Thus, let's create a function to perform the preprocessing of our text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3iNzBxrIf_8U"
      },
      "outputs": [],
      "source": [
        "def GetTreatedText(text_to_treat, nlp):\n",
        "  valid_tokens = []\n",
        "\n",
        "  text_lowered = text_to_treat.lower( )                  # Lowercase\n",
        "\n",
        "  doc = nlp(text_lowered)                                # Getting the doc representation of our text\n",
        "\n",
        "  for token in doc:\n",
        "    if (not token.is_stop) and (token.is_alpha):         # Excluding cases where the token is a stopword or a alphanumeric\n",
        "      valid_tokens.append(token.text)\n",
        "      \n",
        "  if len(valid_tokens) > 2:                              # If there are less than 3 words, return NA\n",
        "    return \" \".join(valid_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_3XOnLikpbW"
      },
      "source": [
        "Nice! Now, let's test our treatment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "dXBIY-cWkoUo",
        "outputId": "9407642d-5238-4288-e49b-1677bee344ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'rio janeiro cidade maravilhosa'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "treated_text = GetTreatedText(text, nlp)\n",
        "treated_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rut2uP0wldOz"
      },
      "source": [
        "Nice! Everything worked out fine. Note that, now, to tokenize our text again, we should simply use a ```split( )``` method. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrWy7Gp_ltCT"
      },
      "source": [
        "# Applying the treatment over our dataset\n",
        "\n",
        "Now, let's perform the treatment over our dataset. First, let's make a basic test with only one of our titles:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "r5uYm6l4k_4I",
        "outputId": "37e9858d-72dc-4be2-d2f2-780d6a793a00"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Após polêmica, Marine Le Pen diz que abomina negacionistas do Holocausto'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "test = df_train.title.iloc[0]\n",
        "test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1XaP5bel4pz"
      },
      "source": [
        "Let's get the treated text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Ys2Bpew5l1qb",
        "outputId": "805d6066-9d95-4982-8ae7-2685a688c8b4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'polêmica marine le pen abomina negacionistas holocausto'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "treated_text = GetTreatedText(test, nlp)\n",
        "treated_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrjGWuRbl9Tq"
      },
      "source": [
        "It seems to have worked once again. So, let's apply to our entire dataframe using apply:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "-5oNJTuvl7d3"
      },
      "outputs": [],
      "source": [
        "df_train['title_treat'] = df_train.title.apply(lambda x : GetTreatedText(x, nlp))\n",
        "df_test['title_treat']  = df_test.title.apply(lambda x : GetTreatedText(x, nlp))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQhEwiAfmPJC"
      },
      "source": [
        "Finally, let's check if everything worked out:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "yefqF6t6mOPl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "657d4592-a8f3-4419-dbc4-6956820b76cf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               title  \\\n",
              "0  Após polêmica, Marine Le Pen diz que abomina n...   \n",
              "1  Macron e Le Pen vão ao 2º turno na França, em ...   \n",
              "2  Apesar de larga vitória nas legislativas, Macr...   \n",
              "3  Governo antecipa balanço, e Alckmin anuncia qu...   \n",
              "4  Após queda em maio, a atividade econômica sobe...   \n",
              "5  Barcelona vence de virada; Atlético de Madri b...   \n",
              "6  'Spartacus' oferece um duplo retrato de batalh...   \n",
              "7  Sobe para 86 o número de mortos no atentado te...   \n",
              "8  Premiada em Sundance, Crystal Moselle retrata ...   \n",
              "9  Metroviários e ferroviários ameaçam parar na p...   \n",
              "\n",
              "                                         title_treat  \n",
              "0  polêmica marine le pen abomina negacionistas h...  \n",
              "1  macron le pen turno frança revés siglas tradic...  \n",
              "2  apesar larga vitória legislativas macron terá ...  \n",
              "3  governo antecipa balanço alckmin anuncia queda...  \n",
              "4       queda maio atividade econômica sobe junho bc  \n",
              "5  barcelona vence virada atlético madri bate bay...  \n",
              "6  spartacus oferece duplo retrato batalhas perdidas  \n",
              "7        sobe mortos atentado terrorista nice frança  \n",
              "8  premiada sundance crystal moselle retrata sexi...  \n",
              "9  metroviários ferroviários ameaçam parar terça ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-79cfdf71-3595-43ea-a60d-cb6221279bba\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>title_treat</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Após polêmica, Marine Le Pen diz que abomina n...</td>\n",
              "      <td>polêmica marine le pen abomina negacionistas h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Macron e Le Pen vão ao 2º turno na França, em ...</td>\n",
              "      <td>macron le pen turno frança revés siglas tradic...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Apesar de larga vitória nas legislativas, Macr...</td>\n",
              "      <td>apesar larga vitória legislativas macron terá ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Governo antecipa balanço, e Alckmin anuncia qu...</td>\n",
              "      <td>governo antecipa balanço alckmin anuncia queda...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Após queda em maio, a atividade econômica sobe...</td>\n",
              "      <td>queda maio atividade econômica sobe junho bc</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Barcelona vence de virada; Atlético de Madri b...</td>\n",
              "      <td>barcelona vence virada atlético madri bate bay...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>'Spartacus' oferece um duplo retrato de batalh...</td>\n",
              "      <td>spartacus oferece duplo retrato batalhas perdidas</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Sobe para 86 o número de mortos no atentado te...</td>\n",
              "      <td>sobe mortos atentado terrorista nice frança</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Premiada em Sundance, Crystal Moselle retrata ...</td>\n",
              "      <td>premiada sundance crystal moselle retrata sexi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Metroviários e ferroviários ameaçam parar na p...</td>\n",
              "      <td>metroviários ferroviários ameaçam parar terça ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-79cfdf71-3595-43ea-a60d-cb6221279bba')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-79cfdf71-3595-43ea-a60d-cb6221279bba button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-79cfdf71-3595-43ea-a60d-cb6221279bba');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "df_train[['title', 'title_treat']].head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nice. Note that, due to our treatment, we may have ended up with some null values. Let's check this:"
      ],
      "metadata": {
        "id": "Ob8HEeRuwSOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['title_treat'].isna( ).sum( )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tIsEa-rwY8g",
        "outputId": "8261f12a-3255-46aa-e15a-c8c0462c99c2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5320"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test['title_treat'].isna( ).sum( )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clHKGpSnwd84",
        "outputId": "afc9a3fc-4fd4-419c-bc44-f62ae31989a2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2178"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indeed, we have some null values in the train and test sets. We can remove those using the ```dropna( )``` method. Also, we can use the ```drop_duplicates( )``` to drop possible duplicated entries. Thus:"
      ],
      "metadata": {
        "id": "s72nUd6NwfIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_train.dropna(subset = ['title_treat']).drop_duplicates( )\n",
        "df_test  = df_test.dropna(subset = ['title_treat']).drop_duplicates( )"
      ],
      "metadata": {
        "id": "QxI6_fnvwqVi"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fitting our Word2Vec model\n",
        "\n",
        "Finally, after we have treated our data, we can fit our Word2Vec model, which will get us our vector representation for each word. We can do that using the ```gensim``` package, using the ```Word2Vec( )```. Some important parameters are:\n",
        "\n",
        "* ```sg```: Boolean. 1 for Skipgram, 0 for Continuous Bag of Words.\n",
        "* ```window```: How many words we be considered before and after our focused word.\n",
        "* ```size```: Size of our final vector.\n",
        "* ```min_count```: Lowest number of frequency necessary for a word to be considered.\n",
        "* ```alpha```: Similar to the Learning Rate from Neural Networks.\n",
        "* ```min_alpha```: ```alpha``` will decay over the epochs, until the model is fitted.\n",
        "\n",
        "Now, let's instance our model:"
      ],
      "metadata": {
        "id": "QowI_XJGrCNO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "gXUEez6imUH1"
      },
      "outputs": [],
      "source": [
        "w2v_model = Word2Vec(sg = 0,\n",
        "                     window = 2,\n",
        "                     size = 300,\n",
        "                     min_count = 3,\n",
        "                     alpha = 0.03,\n",
        "                     min_alpha = 0.007)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before fitting our model, we first have to build our vocabulary. To do so, we use the ```build_vocab( )``` method. This method we receive a list of different lists of tokens. To increase efficiency, we can use a genexp. We can create this list using:"
      ],
      "metadata": {
        "id": "f-u73PhCvJZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_tokens_list = [text.split(\" \") for text in df_train.title_treat]"
      ],
      "metadata": {
        "id": "iGRgEZPUvD_o"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can build our vocabulary. To visualize the building of the vocabulary, we can use a log:"
      ],
      "metadata": {
        "id": "RgAcAAuExaFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(format = \"%(asctime)s : - %(message)s\", level = logging.INFO)\n",
        "w2v_model.build_vocab(list_tokens_list, progress_per = 5000)"
      ],
      "metadata": {
        "id": "etrnZvRGxS2Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29c14b09-6376-4c98-b45b-c1425cbf257a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-08-03 20:29:24,936 : - collecting all words and their counts\n",
            "2022-08-03 20:29:24,942 : - PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2022-08-03 20:29:24,964 : - PROGRESS: at sentence #5000, processed 31930 words, keeping 10193 word types\n",
            "2022-08-03 20:29:24,983 : - PROGRESS: at sentence #10000, processed 63840 words, keeping 14986 word types\n",
            "2022-08-03 20:29:25,003 : - PROGRESS: at sentence #15000, processed 95719 words, keeping 18273 word types\n",
            "2022-08-03 20:29:25,028 : - PROGRESS: at sentence #20000, processed 127647 words, keeping 21016 word types\n",
            "2022-08-03 20:29:25,049 : - PROGRESS: at sentence #25000, processed 159536 words, keeping 23481 word types\n",
            "2022-08-03 20:29:25,068 : - PROGRESS: at sentence #30000, processed 191470 words, keeping 25476 word types\n",
            "2022-08-03 20:29:25,087 : - PROGRESS: at sentence #35000, processed 223330 words, keeping 27311 word types\n",
            "2022-08-03 20:29:25,107 : - PROGRESS: at sentence #40000, processed 255199 words, keeping 29028 word types\n",
            "2022-08-03 20:29:25,126 : - PROGRESS: at sentence #45000, processed 287178 words, keeping 30579 word types\n",
            "2022-08-03 20:29:25,146 : - PROGRESS: at sentence #50000, processed 319105 words, keeping 31946 word types\n",
            "2022-08-03 20:29:25,166 : - PROGRESS: at sentence #55000, processed 351277 words, keeping 33249 word types\n",
            "2022-08-03 20:29:25,185 : - PROGRESS: at sentence #60000, processed 383364 words, keeping 34484 word types\n",
            "2022-08-03 20:29:25,206 : - PROGRESS: at sentence #65000, processed 415356 words, keeping 35603 word types\n",
            "2022-08-03 20:29:25,224 : - PROGRESS: at sentence #70000, processed 447413 words, keeping 36687 word types\n",
            "2022-08-03 20:29:25,246 : - PROGRESS: at sentence #75000, processed 479345 words, keeping 37777 word types\n",
            "2022-08-03 20:29:25,266 : - PROGRESS: at sentence #80000, processed 511363 words, keeping 38786 word types\n",
            "2022-08-03 20:29:25,283 : - collected 39693 word types from a corpus of 541297 raw words and 84680 sentences\n",
            "2022-08-03 20:29:25,286 : - Loading a fresh vocabulary\n",
            "2022-08-03 20:29:25,351 : - effective_min_count=3 retains 18190 unique words (45% of original 39693, drops 21503)\n",
            "2022-08-03 20:29:25,354 : - effective_min_count=3 leaves 514158 word corpus (94% of original 541297, drops 27139)\n",
            "2022-08-03 20:29:25,422 : - deleting the raw counts dictionary of 39693 items\n",
            "2022-08-03 20:29:25,426 : - sample=0.001 downsamples 8 most-common words\n",
            "2022-08-03 20:29:25,430 : - downsampling leaves estimated 505387 word corpus (98.3% of prior 514158)\n",
            "2022-08-03 20:29:25,497 : - estimated required memory for 18190 words and 300 dimensions: 52751000 bytes\n",
            "2022-08-03 20:29:25,500 : - resetting layer weights\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nice! We can see the number of words in our corpus using:"
      ],
      "metadata": {
        "id": "6ybkKfDWVYDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "c_count = w2v_model.corpus_count\n",
        "c_count"
      ],
      "metadata": {
        "id": "DQxeQuXmxeHe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d89acc82-343e-4cc1-efbf-59630ef11e2e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "84680"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can train our Word2Vec model using:"
      ],
      "metadata": {
        "id": "4bBmI4_KVigt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.train(list_tokens_list, total_examples = c_count, epochs = 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n37CzJ_2Vm97",
        "outputId": "1487d76a-3e17-4611-be44-a9fc857644d1"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-08-03 20:29:41,642 : - training model with 3 workers on 18190 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=2\n",
            "2022-08-03 20:29:42,732 : - EPOCH 1 - PROGRESS: at 35.15% examples, 166430 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:29:43,783 : - EPOCH 1 - PROGRESS: at 75.75% examples, 180807 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:29:44,168 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:29:44,172 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:29:44,176 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:29:44,177 : - EPOCH - 1 : training on 541297 raw words (505407 effective words) took 2.5s, 201300 effective words/s\n",
            "2022-08-03 20:29:45,230 : - EPOCH 2 - PROGRESS: at 55.50% examples, 270032 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:29:45,931 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:29:45,948 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:29:45,979 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:29:45,981 : - EPOCH - 2 : training on 541297 raw words (505419 effective words) took 1.8s, 282663 effective words/s\n",
            "2022-08-03 20:29:47,010 : - EPOCH 3 - PROGRESS: at 53.64% examples, 267130 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:29:47,772 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:29:47,781 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:29:47,810 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:29:47,814 : - EPOCH - 3 : training on 541297 raw words (505426 effective words) took 1.8s, 278160 effective words/s\n",
            "2022-08-03 20:29:48,878 : - EPOCH 4 - PROGRESS: at 55.48% examples, 271045 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:29:49,591 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:29:49,605 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:29:49,633 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:29:49,636 : - EPOCH - 4 : training on 541297 raw words (505632 effective words) took 1.8s, 282189 effective words/s\n",
            "2022-08-03 20:29:50,655 : - EPOCH 5 - PROGRESS: at 53.64% examples, 270344 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:29:51,415 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:29:51,464 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:29:51,473 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:29:51,474 : - EPOCH - 5 : training on 541297 raw words (505496 effective words) took 1.8s, 277591 effective words/s\n",
            "2022-08-03 20:29:52,502 : - EPOCH 6 - PROGRESS: at 53.64% examples, 268043 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:29:53,275 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:29:53,292 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:29:53,320 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:29:53,322 : - EPOCH - 6 : training on 541297 raw words (505417 effective words) took 1.8s, 276295 effective words/s\n",
            "2022-08-03 20:29:54,358 : - EPOCH 7 - PROGRESS: at 53.64% examples, 266025 words/s, in_qsize 6, out_qsize 1\n",
            "2022-08-03 20:29:55,081 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:29:55,109 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:29:55,126 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:29:55,131 : - EPOCH - 7 : training on 541297 raw words (505362 effective words) took 1.8s, 282332 effective words/s\n",
            "2022-08-03 20:29:56,192 : - EPOCH 8 - PROGRESS: at 55.48% examples, 269900 words/s, in_qsize 6, out_qsize 0\n",
            "2022-08-03 20:29:57,201 : - EPOCH 8 - PROGRESS: at 96.03% examples, 237234 words/s, in_qsize 3, out_qsize 0\n",
            "2022-08-03 20:29:57,211 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:29:57,236 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:29:57,266 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:29:57,269 : - EPOCH - 8 : training on 541297 raw words (505309 effective words) took 2.1s, 239030 effective words/s\n",
            "2022-08-03 20:29:58,362 : - EPOCH 9 - PROGRESS: at 35.15% examples, 165225 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:29:59,381 : - EPOCH 9 - PROGRESS: at 75.75% examples, 182890 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:30:00,351 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:30:00,379 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:30:00,454 : - EPOCH 9 - PROGRESS: at 100.00% examples, 159683 words/s, in_qsize 0, out_qsize 0\n",
            "2022-08-03 20:30:00,465 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:30:00,467 : - EPOCH - 9 : training on 541297 raw words (505390 effective words) took 3.2s, 159033 effective words/s\n",
            "2022-08-03 20:30:01,522 : - EPOCH 10 - PROGRESS: at 38.85% examples, 193508 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:30:02,529 : - EPOCH 10 - PROGRESS: at 92.35% examples, 231085 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:30:02,582 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:30:02,591 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:30:02,628 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:30:02,630 : - EPOCH - 10 : training on 541297 raw words (505503 effective words) took 2.1s, 238259 effective words/s\n",
            "2022-08-03 20:30:03,709 : - EPOCH 11 - PROGRESS: at 55.48% examples, 263755 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:30:04,418 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:30:04,459 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:30:04,473 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:30:04,475 : - EPOCH - 11 : training on 541297 raw words (505254 effective words) took 1.8s, 276544 effective words/s\n",
            "2022-08-03 20:30:05,562 : - EPOCH 12 - PROGRESS: at 55.50% examples, 262059 words/s, in_qsize 3, out_qsize 2\n",
            "2022-08-03 20:30:06,258 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:30:06,264 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:30:06,280 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:30:06,281 : - EPOCH - 12 : training on 541297 raw words (505249 effective words) took 1.8s, 282689 effective words/s\n",
            "2022-08-03 20:30:07,301 : - EPOCH 13 - PROGRESS: at 53.64% examples, 268982 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:30:08,049 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:30:08,088 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:30:08,090 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:30:08,092 : - EPOCH - 13 : training on 541297 raw words (505422 effective words) took 1.8s, 281180 effective words/s\n",
            "2022-08-03 20:30:09,160 : - EPOCH 14 - PROGRESS: at 64.71% examples, 310068 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:30:09,722 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:30:09,754 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:30:09,769 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:30:09,771 : - EPOCH - 14 : training on 541297 raw words (505445 effective words) took 1.7s, 303755 effective words/s\n",
            "2022-08-03 20:30:10,813 : - EPOCH 15 - PROGRESS: at 55.48% examples, 272597 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:30:11,477 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:30:11,482 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:30:11,519 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:30:11,522 : - EPOCH - 15 : training on 541297 raw words (505314 effective words) took 1.7s, 291138 effective words/s\n",
            "2022-08-03 20:30:12,550 : - EPOCH 16 - PROGRESS: at 61.03% examples, 303387 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:30:13,187 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:30:13,200 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:30:13,203 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:30:13,205 : - EPOCH - 16 : training on 541297 raw words (505432 effective words) took 1.7s, 302594 effective words/s\n",
            "2022-08-03 20:30:14,250 : - EPOCH 17 - PROGRESS: at 55.48% examples, 271973 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:30:14,960 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:30:14,970 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:30:14,989 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:30:14,991 : - EPOCH - 17 : training on 541297 raw words (505423 effective words) took 1.8s, 285556 effective words/s\n",
            "2022-08-03 20:30:16,048 : - EPOCH 18 - PROGRESS: at 55.48% examples, 271076 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:30:16,729 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:30:16,755 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:30:16,771 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:30:16,774 : - EPOCH - 18 : training on 541297 raw words (505378 effective words) took 1.8s, 287227 effective words/s\n",
            "2022-08-03 20:30:17,807 : - EPOCH 19 - PROGRESS: at 55.50% examples, 278995 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:30:18,516 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:30:18,522 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:30:18,536 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:30:18,539 : - EPOCH - 19 : training on 541297 raw words (505453 effective words) took 1.7s, 291095 effective words/s\n",
            "2022-08-03 20:30:19,596 : - EPOCH 20 - PROGRESS: at 57.34% examples, 279064 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:30:20,292 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:30:20,306 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:30:20,324 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:30:20,325 : - EPOCH - 20 : training on 541297 raw words (505357 effective words) took 1.8s, 286060 effective words/s\n",
            "2022-08-03 20:30:21,413 : - EPOCH 21 - PROGRESS: at 49.96% examples, 235972 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:30:22,445 : - EPOCH 21 - PROGRESS: at 88.67% examples, 213369 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:30:22,584 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:30:22,597 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:30:22,609 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:30:22,611 : - EPOCH - 21 : training on 541297 raw words (505416 effective words) took 2.3s, 223073 effective words/s\n",
            "2022-08-03 20:30:23,651 : - EPOCH 22 - PROGRESS: at 59.19% examples, 296272 words/s, in_qsize 6, out_qsize 0\n",
            "2022-08-03 20:30:24,231 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:30:24,251 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:30:24,268 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:30:24,269 : - EPOCH - 22 : training on 541297 raw words (505422 effective words) took 1.6s, 310622 effective words/s\n",
            "2022-08-03 20:30:25,321 : - EPOCH 23 - PROGRESS: at 64.69% examples, 314919 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:30:25,842 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:30:25,856 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:30:25,876 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:30:25,877 : - EPOCH - 23 : training on 541297 raw words (505374 effective words) took 1.6s, 317089 effective words/s\n",
            "2022-08-03 20:30:26,942 : - EPOCH 24 - PROGRESS: at 57.34% examples, 276767 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:30:27,598 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:30:27,626 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:30:27,628 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:30:27,630 : - EPOCH - 24 : training on 541297 raw words (505371 effective words) took 1.7s, 291567 effective words/s\n",
            "2022-08-03 20:30:28,671 : - EPOCH 25 - PROGRESS: at 66.55% examples, 328259 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:30:29,105 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:30:29,117 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:30:29,136 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:30:29,138 : - EPOCH - 25 : training on 541297 raw words (505349 effective words) took 1.5s, 338950 effective words/s\n",
            "2022-08-03 20:30:30,179 : - EPOCH 26 - PROGRESS: at 64.69% examples, 318307 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:30:30,644 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:30:30,652 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:30:30,671 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:30:30,674 : - EPOCH - 26 : training on 541297 raw words (505363 effective words) took 1.5s, 332197 effective words/s\n",
            "2022-08-03 20:30:31,731 : - EPOCH 27 - PROGRESS: at 62.85% examples, 306757 words/s, in_qsize 4, out_qsize 1\n",
            "2022-08-03 20:30:32,281 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:30:32,299 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:30:32,327 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:30:32,328 : - EPOCH - 27 : training on 541297 raw words (505438 effective words) took 1.6s, 309796 effective words/s\n",
            "2022-08-03 20:30:33,357 : - EPOCH 28 - PROGRESS: at 55.48% examples, 276071 words/s, in_qsize 6, out_qsize 0\n",
            "2022-08-03 20:30:34,093 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:30:34,096 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:30:34,098 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:30:34,101 : - EPOCH - 28 : training on 541297 raw words (505354 effective words) took 1.8s, 287437 effective words/s\n",
            "2022-08-03 20:30:35,181 : - EPOCH 29 - PROGRESS: at 57.34% examples, 273997 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:30:35,815 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:30:35,821 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:30:35,836 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:30:35,838 : - EPOCH - 29 : training on 541297 raw words (505436 effective words) took 1.7s, 295124 effective words/s\n",
            "2022-08-03 20:30:36,925 : - EPOCH 30 - PROGRESS: at 68.39% examples, 324169 words/s, in_qsize 6, out_qsize 0\n",
            "2022-08-03 20:30:37,310 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:30:37,318 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:30:37,334 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:30:37,335 : - EPOCH - 30 : training on 541297 raw words (505421 effective words) took 1.5s, 342584 effective words/s\n",
            "2022-08-03 20:30:37,338 : - training on a 16238910 raw words (15162032 effective words) took 55.7s, 272244 effective words/s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15162032, 16238910)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training of our first model took 55.7 s."
      ],
      "metadata": {
        "id": "hlKOUm9CZotw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating our model\n",
        "\n",
        "To see if our model is good, we can try to evaluate the similarity between words. For instance, if we pass \"google\" to our model, we expect the model to return the most similar words (which will likely be related to big tech companies. Let's test it:"
      ],
      "metadata": {
        "id": "DYYPD8saWp0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.wv.most_similar(\"google\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4SLjlEkW7Kq",
        "outputId": "2a8008bf-e04c-47c7-f373-8e27012f3e86"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-08-03 20:30:51,343 : - precomputing L2-norms of word weight vectors\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('apple', 0.591137707233429),\n",
              " ('amazon', 0.5327430963516235),\n",
              " ('facebook', 0.518150269985199),\n",
              " ('disney', 0.5077775120735168),\n",
              " ('tesla', 0.49346739053726196),\n",
              " ('airbnb', 0.49191713333129883),\n",
              " ('alibaba', 0.4917002022266388),\n",
              " ('volkswagen', 0.48766130208969116),\n",
              " ('yahoo', 0.48442187905311584),\n",
              " ('uber', 0.4781746566295624)]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nice! We really got what we would expect. Let's test another word:"
      ],
      "metadata": {
        "id": "XDMIoKPUXnvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.wv.most_similar(\"china\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4JeXv2AXvTd",
        "outputId": "95638ce0-8539-48a8-9edb-19623abb72fc"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('rock', 0.40720683336257935),\n",
              " ('índia', 0.37196022272109985),\n",
              " ('chinesa', 0.3669336140155792),\n",
              " ('expedia', 0.35940343141555786),\n",
              " ('toshiba', 0.3531379699707031),\n",
              " ('méxico', 0.35062330961227417),\n",
              " ('malvinas', 0.34907084703445435),\n",
              " ('bc', 0.3440648913383484),\n",
              " ('bce', 0.34233248233795166),\n",
              " ('otimismo', 0.3391525149345398)]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that, for \"china\", the words are not very common. The most similar word is \"rock\", with \"india\" comming right after (which makes more sense). "
      ],
      "metadata": {
        "id": "RBdNr0tTXxnP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fitting another Word2Vec model\n",
        "\n",
        "Note that, here, our model was based on a continuous bag of words (CBOW). However, we can also fit a SKIPGRAM to our data. To do so, we can instance a different model:"
      ],
      "metadata": {
        "id": "xktYG-5ZX8km"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_skipg = Word2Vec(sg = 1,\n",
        "                     window = 5,\n",
        "                     size = 300,\n",
        "                     min_count = 3,\n",
        "                     alpha = 0.03,\n",
        "                     min_alpha = 0.007)\n",
        "\n",
        "w2v_skipg.build_vocab(list_tokens_list, progress_per = 5000)\n",
        "\n",
        "c_count = w2v_skipg.corpus_count\n",
        "\n",
        "w2v_skipg.train(list_tokens_list, total_examples = c_count, epochs = 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIe-OTLBYJLf",
        "outputId": "c3b22b9e-555c-4a1c-c7bb-4ae97c433eaf"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-08-03 20:31:45,091 : - collecting all words and their counts\n",
            "2022-08-03 20:31:45,096 : - PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2022-08-03 20:31:45,115 : - PROGRESS: at sentence #5000, processed 31930 words, keeping 10193 word types\n",
            "2022-08-03 20:31:45,132 : - PROGRESS: at sentence #10000, processed 63840 words, keeping 14986 word types\n",
            "2022-08-03 20:31:45,149 : - PROGRESS: at sentence #15000, processed 95719 words, keeping 18273 word types\n",
            "2022-08-03 20:31:45,167 : - PROGRESS: at sentence #20000, processed 127647 words, keeping 21016 word types\n",
            "2022-08-03 20:31:45,187 : - PROGRESS: at sentence #25000, processed 159536 words, keeping 23481 word types\n",
            "2022-08-03 20:31:45,209 : - PROGRESS: at sentence #30000, processed 191470 words, keeping 25476 word types\n",
            "2022-08-03 20:31:45,230 : - PROGRESS: at sentence #35000, processed 223330 words, keeping 27311 word types\n",
            "2022-08-03 20:31:45,250 : - PROGRESS: at sentence #40000, processed 255199 words, keeping 29028 word types\n",
            "2022-08-03 20:31:45,271 : - PROGRESS: at sentence #45000, processed 287178 words, keeping 30579 word types\n",
            "2022-08-03 20:31:45,292 : - PROGRESS: at sentence #50000, processed 319105 words, keeping 31946 word types\n",
            "2022-08-03 20:31:45,313 : - PROGRESS: at sentence #55000, processed 351277 words, keeping 33249 word types\n",
            "2022-08-03 20:31:45,336 : - PROGRESS: at sentence #60000, processed 383364 words, keeping 34484 word types\n",
            "2022-08-03 20:31:45,358 : - PROGRESS: at sentence #65000, processed 415356 words, keeping 35603 word types\n",
            "2022-08-03 20:31:45,379 : - PROGRESS: at sentence #70000, processed 447413 words, keeping 36687 word types\n",
            "2022-08-03 20:31:45,401 : - PROGRESS: at sentence #75000, processed 479345 words, keeping 37777 word types\n",
            "2022-08-03 20:31:45,423 : - PROGRESS: at sentence #80000, processed 511363 words, keeping 38786 word types\n",
            "2022-08-03 20:31:45,443 : - collected 39693 word types from a corpus of 541297 raw words and 84680 sentences\n",
            "2022-08-03 20:31:45,448 : - Loading a fresh vocabulary\n",
            "2022-08-03 20:31:45,509 : - effective_min_count=3 retains 18190 unique words (45% of original 39693, drops 21503)\n",
            "2022-08-03 20:31:45,511 : - effective_min_count=3 leaves 514158 word corpus (94% of original 541297, drops 27139)\n",
            "2022-08-03 20:31:45,589 : - deleting the raw counts dictionary of 39693 items\n",
            "2022-08-03 20:31:45,594 : - sample=0.001 downsamples 8 most-common words\n",
            "2022-08-03 20:31:45,599 : - downsampling leaves estimated 505387 word corpus (98.3% of prior 514158)\n",
            "2022-08-03 20:31:45,711 : - estimated required memory for 18190 words and 300 dimensions: 52751000 bytes\n",
            "2022-08-03 20:31:45,715 : - resetting layer weights\n",
            "2022-08-03 20:31:49,657 : - training model with 3 workers on 18190 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
            "2022-08-03 20:31:50,844 : - EPOCH 1 - PROGRESS: at 22.20% examples, 95523 words/s, in_qsize 4, out_qsize 1\n",
            "2022-08-03 20:31:51,851 : - EPOCH 1 - PROGRESS: at 46.26% examples, 107037 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:31:52,931 : - EPOCH 1 - PROGRESS: at 68.39% examples, 105924 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:31:53,945 : - EPOCH 1 - PROGRESS: at 90.51% examples, 107017 words/s, in_qsize 4, out_qsize 1\n",
            "2022-08-03 20:31:54,185 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:31:54,231 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:31:54,274 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:31:54,277 : - EPOCH - 1 : training on 541297 raw words (505437 effective words) took 4.6s, 109750 effective words/s\n",
            "2022-08-03 20:31:55,354 : - EPOCH 2 - PROGRESS: at 22.20% examples, 108920 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:31:56,400 : - EPOCH 2 - PROGRESS: at 44.41% examples, 106711 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:31:57,445 : - EPOCH 2 - PROGRESS: at 66.53% examples, 106816 words/s, in_qsize 4, out_qsize 1\n",
            "2022-08-03 20:31:58,470 : - EPOCH 2 - PROGRESS: at 90.49% examples, 109690 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:31:58,703 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:31:58,784 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:31:58,810 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:31:58,812 : - EPOCH - 2 : training on 541297 raw words (505309 effective words) took 4.5s, 112008 effective words/s\n",
            "2022-08-03 20:31:59,967 : - EPOCH 3 - PROGRESS: at 22.20% examples, 98481 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:32:01,089 : - EPOCH 3 - PROGRESS: at 46.26% examples, 103280 words/s, in_qsize 4, out_qsize 1\n",
            "2022-08-03 20:32:02,194 : - EPOCH 3 - PROGRESS: at 72.07% examples, 108197 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:32:03,294 : - EPOCH 3 - PROGRESS: at 96.03% examples, 108717 words/s, in_qsize 3, out_qsize 0\n",
            "2022-08-03 20:32:03,351 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:32:03,364 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:32:03,390 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:32:03,391 : - EPOCH - 3 : training on 541297 raw words (505422 effective words) took 4.6s, 110782 effective words/s\n",
            "2022-08-03 20:32:04,419 : - EPOCH 4 - PROGRESS: at 20.35% examples, 101585 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:32:05,587 : - EPOCH 4 - PROGRESS: at 44.41% examples, 102800 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:32:06,593 : - EPOCH 4 - PROGRESS: at 66.55% examples, 105428 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:32:07,643 : - EPOCH 4 - PROGRESS: at 90.49% examples, 107965 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:32:07,874 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:32:08,023 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:32:08,026 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:32:08,031 : - EPOCH - 4 : training on 541297 raw words (505255 effective words) took 4.6s, 109284 effective words/s\n",
            "2022-08-03 20:32:09,063 : - EPOCH 5 - PROGRESS: at 20.35% examples, 101979 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:32:10,089 : - EPOCH 5 - PROGRESS: at 42.57% examples, 105619 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:32:11,176 : - EPOCH 5 - PROGRESS: at 66.53% examples, 107678 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:32:12,278 : - EPOCH 5 - PROGRESS: at 90.49% examples, 108331 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:32:12,506 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:32:12,532 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:32:12,574 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:32:12,576 : - EPOCH - 5 : training on 541297 raw words (505516 effective words) took 4.5s, 111828 effective words/s\n",
            "2022-08-03 20:32:13,629 : - EPOCH 6 - PROGRESS: at 20.35% examples, 99146 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:32:14,649 : - EPOCH 6 - PROGRESS: at 44.41% examples, 108949 words/s, in_qsize 4, out_qsize 1\n",
            "2022-08-03 20:32:15,843 : - EPOCH 6 - PROGRESS: at 72.05% examples, 111984 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:32:16,855 : - EPOCH 6 - PROGRESS: at 94.17% examples, 111709 words/s, in_qsize 3, out_qsize 1\n",
            "2022-08-03 20:32:16,898 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:32:16,909 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:32:17,010 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:32:17,012 : - EPOCH - 6 : training on 541297 raw words (505476 effective words) took 4.4s, 114375 effective words/s\n",
            "2022-08-03 20:32:18,044 : - EPOCH 7 - PROGRESS: at 20.35% examples, 101107 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:32:19,159 : - EPOCH 7 - PROGRESS: at 44.41% examples, 105139 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:32:20,208 : - EPOCH 7 - PROGRESS: at 70.21% examples, 111494 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:32:21,348 : - EPOCH 7 - PROGRESS: at 94.17% examples, 110173 words/s, in_qsize 3, out_qsize 1\n",
            "2022-08-03 20:32:21,373 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:32:21,391 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:32:21,488 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:32:21,490 : - EPOCH - 7 : training on 541297 raw words (505359 effective words) took 4.5s, 113257 effective words/s\n",
            "2022-08-03 20:32:22,654 : - EPOCH 8 - PROGRESS: at 22.20% examples, 97917 words/s, in_qsize 6, out_qsize 0\n",
            "2022-08-03 20:32:23,663 : - EPOCH 8 - PROGRESS: at 44.41% examples, 104049 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:32:24,791 : - EPOCH 8 - PROGRESS: at 70.21% examples, 108301 words/s, in_qsize 5, out_qsize 1\n",
            "2022-08-03 20:32:25,804 : - EPOCH 8 - PROGRESS: at 94.20% examples, 110838 words/s, in_qsize 4, out_qsize 0\n",
            "2022-08-03 20:32:25,946 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:32:25,956 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:32:25,974 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:32:25,976 : - EPOCH - 8 : training on 541297 raw words (505352 effective words) took 4.5s, 113144 effective words/s\n",
            "2022-08-03 20:32:27,086 : - EPOCH 9 - PROGRESS: at 20.35% examples, 94330 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:32:28,117 : - EPOCH 9 - PROGRESS: at 46.25% examples, 110101 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:32:29,142 : - EPOCH 9 - PROGRESS: at 70.21% examples, 112776 words/s, in_qsize 4, out_qsize 1\n",
            "2022-08-03 20:32:30,143 : - EPOCH 9 - PROGRESS: at 96.31% examples, 117435 words/s, in_qsize 2, out_qsize 1\n",
            "2022-08-03 20:32:30,148 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:32:30,179 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:32:30,230 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:32:30,232 : - EPOCH - 9 : training on 541297 raw words (505542 effective words) took 4.2s, 119379 effective words/s\n",
            "2022-08-03 20:32:31,315 : - EPOCH 10 - PROGRESS: at 20.35% examples, 96110 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:32:32,416 : - EPOCH 10 - PROGRESS: at 44.41% examples, 103268 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:32:33,537 : - EPOCH 10 - PROGRESS: at 72.05% examples, 110581 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:32:34,472 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:32:34,532 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:32:34,545 : - EPOCH 10 - PROGRESS: at 100.00% examples, 117585 words/s, in_qsize 0, out_qsize 1\n",
            "2022-08-03 20:32:34,546 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:32:34,548 : - EPOCH - 10 : training on 541297 raw words (505355 effective words) took 4.3s, 117500 effective words/s\n",
            "2022-08-03 20:32:35,599 : - EPOCH 11 - PROGRESS: at 20.35% examples, 99415 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:32:36,602 : - EPOCH 11 - PROGRESS: at 42.55% examples, 105429 words/s, in_qsize 6, out_qsize 0\n",
            "2022-08-03 20:32:37,676 : - EPOCH 11 - PROGRESS: at 66.55% examples, 108018 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:32:38,808 : - EPOCH 11 - PROGRESS: at 92.33% examples, 110018 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:32:39,014 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:32:39,083 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:32:39,142 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:32:39,144 : - EPOCH - 11 : training on 541297 raw words (505398 effective words) took 4.6s, 110389 effective words/s\n",
            "2022-08-03 20:32:40,193 : - EPOCH 12 - PROGRESS: at 11.10% examples, 54685 words/s, in_qsize 6, out_qsize 0\n",
            "2022-08-03 20:32:41,249 : - EPOCH 12 - PROGRESS: at 22.20% examples, 55045 words/s, in_qsize 6, out_qsize 0\n",
            "2022-08-03 20:32:42,215 : - EPOCH 12 - PROGRESS: at 33.29% examples, 55160 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:32:43,314 : - EPOCH 12 - PROGRESS: at 46.25% examples, 56290 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:32:44,369 : - EPOCH 12 - PROGRESS: at 70.21% examples, 68211 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:32:45,392 : - EPOCH 12 - PROGRESS: at 92.33% examples, 74997 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:32:45,536 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:32:45,593 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:32:45,627 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:32:45,629 : - EPOCH - 12 : training on 541297 raw words (505436 effective words) took 6.5s, 78244 effective words/s\n",
            "2022-08-03 20:32:46,706 : - EPOCH 13 - PROGRESS: at 20.35% examples, 97251 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:32:47,734 : - EPOCH 13 - PROGRESS: at 42.57% examples, 102987 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:32:48,744 : - EPOCH 13 - PROGRESS: at 66.55% examples, 108558 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:32:49,795 : - EPOCH 13 - PROGRESS: at 92.33% examples, 112573 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:32:50,014 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:32:50,024 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:32:50,073 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:32:50,077 : - EPOCH - 13 : training on 541297 raw words (505467 effective words) took 4.4s, 114160 effective words/s\n",
            "2022-08-03 20:32:51,218 : - EPOCH 14 - PROGRESS: at 22.20% examples, 99690 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:32:52,221 : - EPOCH 14 - PROGRESS: at 44.41% examples, 105365 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:32:53,316 : - EPOCH 14 - PROGRESS: at 68.39% examples, 107197 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:32:54,360 : - EPOCH 14 - PROGRESS: at 92.33% examples, 109417 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:32:54,557 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:32:54,579 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:32:54,655 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:32:54,656 : - EPOCH - 14 : training on 541297 raw words (505410 effective words) took 4.6s, 110789 effective words/s\n",
            "2022-08-03 20:32:55,765 : - EPOCH 15 - PROGRESS: at 24.04% examples, 111141 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:32:56,769 : - EPOCH 15 - PROGRESS: at 49.96% examples, 120262 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:32:57,817 : - EPOCH 15 - PROGRESS: at 73.91% examples, 118782 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:32:58,864 : - EPOCH 15 - PROGRESS: at 96.04% examples, 115837 words/s, in_qsize 3, out_qsize 0\n",
            "2022-08-03 20:32:58,885 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:32:58,902 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:32:58,944 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:32:58,947 : - EPOCH - 15 : training on 541297 raw words (505470 effective words) took 4.3s, 118266 effective words/s\n",
            "2022-08-03 20:32:59,994 : - EPOCH 16 - PROGRESS: at 20.35% examples, 99954 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:33:01,062 : - EPOCH 16 - PROGRESS: at 46.25% examples, 111295 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:33:02,130 : - EPOCH 16 - PROGRESS: at 70.21% examples, 112082 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:33:03,175 : - EPOCH 16 - PROGRESS: at 96.03% examples, 115309 words/s, in_qsize 3, out_qsize 0\n",
            "2022-08-03 20:33:03,202 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:33:03,221 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:33:03,294 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:33:03,296 : - EPOCH - 16 : training on 541297 raw words (505408 effective words) took 4.3s, 116711 effective words/s\n",
            "2022-08-03 20:33:04,326 : - EPOCH 17 - PROGRESS: at 20.35% examples, 102730 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:33:05,387 : - EPOCH 17 - PROGRESS: at 46.26% examples, 113222 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:33:06,465 : - EPOCH 17 - PROGRESS: at 70.21% examples, 112936 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:33:07,547 : - EPOCH 17 - PROGRESS: at 92.33% examples, 110539 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:33:07,616 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:33:07,760 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:33:07,775 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:33:07,779 : - EPOCH - 17 : training on 541297 raw words (505341 effective words) took 4.5s, 113488 effective words/s\n",
            "2022-08-03 20:33:08,813 : - EPOCH 18 - PROGRESS: at 18.50% examples, 92922 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:33:09,954 : - EPOCH 18 - PROGRESS: at 44.42% examples, 104431 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:33:10,984 : - EPOCH 18 - PROGRESS: at 68.39% examples, 108701 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:33:11,987 : - EPOCH 18 - PROGRESS: at 90.49% examples, 109427 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:33:12,206 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:33:12,257 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:33:12,297 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:33:12,298 : - EPOCH - 18 : training on 541297 raw words (505348 effective words) took 4.5s, 112563 effective words/s\n",
            "2022-08-03 20:33:13,338 : - EPOCH 19 - PROGRESS: at 18.50% examples, 92898 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:33:14,387 : - EPOCH 19 - PROGRESS: at 38.85% examples, 97255 words/s, in_qsize 6, out_qsize 0\n",
            "2022-08-03 20:33:15,378 : - EPOCH 19 - PROGRESS: at 62.85% examples, 104200 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:33:16,477 : - EPOCH 19 - PROGRESS: at 86.82% examples, 105817 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:33:16,780 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:33:16,881 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:33:16,885 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:33:16,886 : - EPOCH - 19 : training on 541297 raw words (505383 effective words) took 4.6s, 110971 effective words/s\n",
            "2022-08-03 20:33:17,908 : - EPOCH 20 - PROGRESS: at 22.20% examples, 111420 words/s, in_qsize 6, out_qsize 0\n",
            "2022-08-03 20:33:19,035 : - EPOCH 20 - PROGRESS: at 46.25% examples, 109429 words/s, in_qsize 4, out_qsize 1\n",
            "2022-08-03 20:33:20,144 : - EPOCH 20 - PROGRESS: at 70.22% examples, 109398 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:33:21,279 : - EPOCH 20 - PROGRESS: at 94.17% examples, 108789 words/s, in_qsize 4, out_qsize 0\n",
            "2022-08-03 20:33:21,314 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:33:21,328 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:33:21,424 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:33:21,426 : - EPOCH - 20 : training on 541297 raw words (505387 effective words) took 4.5s, 111720 effective words/s\n",
            "2022-08-03 20:33:22,452 : - EPOCH 21 - PROGRESS: at 20.35% examples, 101385 words/s, in_qsize 6, out_qsize 0\n",
            "2022-08-03 20:33:23,540 : - EPOCH 21 - PROGRESS: at 44.41% examples, 106604 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:33:24,555 : - EPOCH 21 - PROGRESS: at 68.39% examples, 110808 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:33:25,587 : - EPOCH 21 - PROGRESS: at 94.20% examples, 114758 words/s, in_qsize 4, out_qsize 0\n",
            "2022-08-03 20:33:25,663 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:33:25,684 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:33:25,744 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:33:25,746 : - EPOCH - 21 : training on 541297 raw words (505372 effective words) took 4.3s, 117352 effective words/s\n",
            "2022-08-03 20:33:26,775 : - EPOCH 22 - PROGRESS: at 18.51% examples, 91948 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:33:27,781 : - EPOCH 22 - PROGRESS: at 33.29% examples, 83121 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:33:28,795 : - EPOCH 22 - PROGRESS: at 51.80% examples, 86071 words/s, in_qsize 6, out_qsize 0\n",
            "2022-08-03 20:33:29,816 : - EPOCH 22 - PROGRESS: at 66.55% examples, 82823 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:33:30,852 : - EPOCH 22 - PROGRESS: at 84.99% examples, 84295 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:33:31,550 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:33:31,609 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:33:31,697 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:33:31,704 : - EPOCH - 22 : training on 541297 raw words (505445 effective words) took 5.9s, 85022 effective words/s\n",
            "2022-08-03 20:33:32,802 : - EPOCH 23 - PROGRESS: at 24.05% examples, 112745 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:33:33,894 : - EPOCH 23 - PROGRESS: at 53.64% examples, 124830 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:33:34,929 : - EPOCH 23 - PROGRESS: at 79.44% examples, 125283 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:33:35,668 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:33:35,686 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:33:35,743 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:33:35,745 : - EPOCH - 23 : training on 541297 raw words (505420 effective words) took 4.0s, 125747 effective words/s\n",
            "2022-08-03 20:33:36,778 : - EPOCH 24 - PROGRESS: at 24.05% examples, 120590 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:33:37,801 : - EPOCH 24 - PROGRESS: at 49.95% examples, 124145 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:33:38,942 : - EPOCH 24 - PROGRESS: at 79.44% examples, 126604 words/s, in_qsize 4, out_qsize 1\n",
            "2022-08-03 20:33:39,544 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:33:39,566 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:33:39,607 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:33:39,608 : - EPOCH - 24 : training on 541297 raw words (505417 effective words) took 3.8s, 131754 effective words/s\n",
            "2022-08-03 20:33:40,653 : - EPOCH 25 - PROGRESS: at 24.05% examples, 118156 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:33:41,656 : - EPOCH 25 - PROGRESS: at 49.95% examples, 124133 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:33:42,698 : - EPOCH 25 - PROGRESS: at 77.59% examples, 127558 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:33:43,411 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:33:43,422 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:33:43,486 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:33:43,487 : - EPOCH - 25 : training on 541297 raw words (505341 effective words) took 3.9s, 130862 effective words/s\n",
            "2022-08-03 20:33:44,596 : - EPOCH 26 - PROGRESS: at 25.89% examples, 119775 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:33:45,643 : - EPOCH 26 - PROGRESS: at 55.48% examples, 130966 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:33:46,691 : - EPOCH 26 - PROGRESS: at 79.44% examples, 125994 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:33:47,478 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:33:47,516 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:33:47,567 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:33:47,570 : - EPOCH - 26 : training on 541297 raw words (505435 effective words) took 4.1s, 124332 effective words/s\n",
            "2022-08-03 20:33:48,833 : - EPOCH 27 - PROGRESS: at 24.04% examples, 98743 words/s, in_qsize 6, out_qsize 1\n",
            "2022-08-03 20:33:49,847 : - EPOCH 27 - PROGRESS: at 48.11% examples, 108148 words/s, in_qsize 4, out_qsize 1\n",
            "2022-08-03 20:33:50,906 : - EPOCH 27 - PROGRESS: at 72.05% examples, 110205 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:33:51,920 : - EPOCH 27 - PROGRESS: at 94.20% examples, 110282 words/s, in_qsize 4, out_qsize 0\n",
            "2022-08-03 20:33:52,009 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:33:52,022 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:33:52,098 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:33:52,100 : - EPOCH - 27 : training on 541297 raw words (505305 effective words) took 4.5s, 112397 effective words/s\n",
            "2022-08-03 20:33:53,186 : - EPOCH 28 - PROGRESS: at 24.04% examples, 113005 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:33:54,354 : - EPOCH 28 - PROGRESS: at 53.64% examples, 120736 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:33:55,391 : - EPOCH 28 - PROGRESS: at 79.44% examples, 122371 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:33:56,134 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:33:56,147 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:33:56,249 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:33:56,251 : - EPOCH - 28 : training on 541297 raw words (505357 effective words) took 4.1s, 122124 effective words/s\n",
            "2022-08-03 20:33:57,396 : - EPOCH 29 - PROGRESS: at 22.20% examples, 99153 words/s, in_qsize 4, out_qsize 1\n",
            "2022-08-03 20:33:58,421 : - EPOCH 29 - PROGRESS: at 49.96% examples, 116929 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:33:59,448 : - EPOCH 29 - PROGRESS: at 73.90% examples, 117307 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:34:00,442 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:34:00,469 : - EPOCH 29 - PROGRESS: at 98.16% examples, 118031 words/s, in_qsize 1, out_qsize 1\n",
            "2022-08-03 20:34:00,472 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:34:00,486 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:34:00,488 : - EPOCH - 29 : training on 541297 raw words (505289 effective words) took 4.2s, 119704 effective words/s\n",
            "2022-08-03 20:34:01,567 : - EPOCH 30 - PROGRESS: at 22.20% examples, 105335 words/s, in_qsize 5, out_qsize 0\n",
            "2022-08-03 20:34:02,628 : - EPOCH 30 - PROGRESS: at 44.41% examples, 105444 words/s, in_qsize 4, out_qsize 1\n",
            "2022-08-03 20:34:03,744 : - EPOCH 30 - PROGRESS: at 70.22% examples, 109399 words/s, in_qsize 4, out_qsize 1\n",
            "2022-08-03 20:34:04,807 : - EPOCH 30 - PROGRESS: at 96.04% examples, 112769 words/s, in_qsize 3, out_qsize 0\n",
            "2022-08-03 20:34:04,827 : - worker thread finished; awaiting finish of 2 more threads\n",
            "2022-08-03 20:34:04,842 : - worker thread finished; awaiting finish of 1 more threads\n",
            "2022-08-03 20:34:04,912 : - worker thread finished; awaiting finish of 0 more threads\n",
            "2022-08-03 20:34:04,913 : - EPOCH - 30 : training on 541297 raw words (505312 effective words) took 4.4s, 114595 effective words/s\n",
            "2022-08-03 20:34:04,919 : - training on a 16238910 raw words (15161764 effective words) took 135.3s, 112094 effective words/s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15161764, 16238910)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the Skipgram model has a higher training time. Let's see the most similar words once again:"
      ],
      "metadata": {
        "id": "ZoIViKNJY4tE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_skipg.wv.most_similar(\"google\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3jC1jd7ZDu6",
        "outputId": "38bdfff6-d6ca-46f9-bd34-a4ee8c43242a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-08-03 20:34:29,717 : - precomputing L2-norms of word weight vectors\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('alphabet', 0.4390574097633362),\n",
              " ('antitruste', 0.43469756841659546),\n",
              " ('android', 0.4245913624763489),\n",
              " ('reguladores', 0.4116258919239044),\n",
              " ('waze', 0.4096982181072235),\n",
              " ('paypal', 0.40019655227661133),\n",
              " ('lyft', 0.3987908363342285),\n",
              " ('apple', 0.39382725954055786),\n",
              " ('difusão', 0.38902547955513),\n",
              " ('buffett', 0.3868878483772278)]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that, this time, we did not get big tech companies. We got alphabet (which is the name of the company that owns google), antitruste (which is related to a law which directly targets google), android, and others.\n",
        "\n",
        "Let's check china:"
      ],
      "metadata": {
        "id": "rgzCe4JQan6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_skipg.wv.most_similar(\"china\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Y9W0a5NZHr_",
        "outputId": "5ede99f9-4d1f-4964-c28f-bf88bb79a689"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('town', 0.4401307702064514),\n",
              " ('desvalorizar', 0.4094200134277344),\n",
              " ('taiwan', 0.3961268663406372),\n",
              " ('chinês', 0.3897581100463867),\n",
              " ('expedia', 0.38523411750793457),\n",
              " ('yuan', 0.38365963101387024),\n",
              " ('felipão', 0.37362807989120483),\n",
              " ('estabilização', 0.3668305575847626),\n",
              " ('bilateral', 0.36438506841659546),\n",
              " ('sobrevoam', 0.36179083585739136)]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, we got different words, such as \"town\" (likely from chinatown), \"desvalorizar\", \"taiwan\", and others.\n",
        "\n",
        "This occurs because the Skipgram model takes into consideration more information about the context of the word. "
      ],
      "metadata": {
        "id": "HBNvf21Vab5e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving our Word2Vec models\n",
        "\n",
        "Finally, to save our model, we can do:"
      ],
      "metadata": {
        "id": "3DpSq95gbOym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.wv.save_word2vec_format(\"/content/drive/MyDrive/Colab Notebooks/Machine Learning/Avançado/model_cbow.txt\", binary = False)\n",
        "w2v_skipg.wv.save_word2vec_format(\"/content/drive/MyDrive/Colab Notebooks/Machine Learning/Avançado/model_skip.txt\", binary = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqRV0jfTbMhD",
        "outputId": "3eb25aeb-6723-4e45-f650-ed09813a57d5"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-08-03 20:38:33,880 : - storing 18190x300 projection weights into /content/drive/MyDrive/Colab Notebooks/Machine Learning/Avançado/model_cbow.txt\n",
            "2022-08-03 20:38:39,196 : - storing 18190x300 projection weights into /content/drive/MyDrive/Colab Notebooks/Machine Learning/Avançado/model_skip.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get our models, we can also do:"
      ],
      "metadata": {
        "id": "QtPmPM0jg6OX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_cbow = KeyedVectors.load_word2vec_format(\"/content/drive/MyDrive/Colab Notebooks/Machine Learning/Avançado/model_cbow.txt\")\n",
        "w2v_skip = KeyedVectors.load_word2vec_format(\"/content/drive/MyDrive/Colab Notebooks/Machine Learning/Avançado/model_skip.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63M1hIwpg80k",
        "outputId": "9450930a-9993-47cc-adbb-98d809842f26"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-08-03 21:02:56,581 : - loading projection weights from /content/drive/MyDrive/Colab Notebooks/Machine Learning/Avançado/model_cbow.txt\n",
            "2022-08-03 21:03:01,941 : - loaded (18190, 300) matrix from /content/drive/MyDrive/Colab Notebooks/Machine Learning/Avançado/model_cbow.txt\n",
            "2022-08-03 21:03:01,944 : - loading projection weights from /content/drive/MyDrive/Colab Notebooks/Machine Learning/Avançado/model_skip.txt\n",
            "2022-08-03 21:03:05,036 : - loaded (18190, 300) matrix from /content/drive/MyDrive/Colab Notebooks/Machine Learning/Avançado/model_skip.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using our new model to assist in our classification\n",
        "\n",
        "Finally, we have built a Word2Vec model. This model is able to make a treatment in our text, creating a vectorized representation of our text. Then, we are able to fit a model to this vector.\n",
        "\n",
        "Our Word2Vec model returns a vectorized representation of a word. There are many approaches that we can do get a vectorized representation of an entire text. The most simple approach is to simply sum all vectors for the words in our text. Let's define a function to get the vectorized representation of a text:"
      ],
      "metadata": {
        "id": "2BlCPLyKdn2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def SumCombination(treated_text, w2v_model, nlp):\n",
        "  resulting_vector = np.zeros(300)               # Initializing our vector\n",
        "\n",
        "  tokens = treated_text.split(' ')               # Getting the tokens from our text\n",
        "  \n",
        "  for token in tokens:\n",
        "    try:\n",
        "      resulting_vector += w2v_model.get_vector(token)\n",
        "    except KeyError:\n",
        "      pass\n",
        "\n",
        "  return resulting_vector"
      ],
      "metadata": {
        "id": "xSVhkgqubm_f"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try an example:"
      ],
      "metadata": {
        "id": "UqELpXjZe-hz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example = df_train.title_treat.iloc[0]\n",
        "example"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "k44hdFobeXK3",
        "outputId": "8cce994e-9b52-4cbc-c227-812cd6fcb3f3"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'polêmica marine le pen abomina negacionistas holocausto'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SumCombination(example, w2v_cbow, nlp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ejzvaWrfOBY",
        "outputId": "92c2d344-2d3e-4c96-e3f3-32063e15d6d5"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.1639581 ,  0.99361866,  1.58946272, -1.70519099, -0.58655722,\n",
              "        2.76381978, -1.48856495,  2.61652827, -2.8249495 , -1.11621271,\n",
              "       -1.21121646, -0.35616846, -1.81881744, -4.00770722, -0.69931941,\n",
              "        1.52200159,  1.20268416, -1.00832912,  4.1725359 , -0.66098908,\n",
              "        2.01742315, -1.92499796,  0.29944097,  0.5762695 , -3.5113394 ,\n",
              "       -1.78334784,  2.60668474, -0.52663851,  4.239319  ,  2.74070652,\n",
              "       -0.5602636 ,  0.58146892,  0.46159234,  1.89427415, -0.50434637,\n",
              "        1.54227377,  1.70399833, -0.57121015, -1.33123249,  2.57822677,\n",
              "        0.66038442,  1.21749611, -1.81092528, -2.21237579,  1.45097527,\n",
              "        0.39411859,  1.69874291, -0.18345772,  1.64049189, -0.52350726,\n",
              "        0.43626305, -1.25838883,  1.78670114, -0.8610464 ,  0.67001021,\n",
              "        0.42579739,  0.62606178, -0.14037363, -0.9631228 ,  0.7100436 ,\n",
              "       -2.80344407,  1.17153793,  1.04110266, -2.50932276, -1.76449172,\n",
              "        0.17380876,  1.36496691, -0.91505472,  1.61307287,  1.19283125,\n",
              "        1.65286767, -1.12312995,  1.14199309,  0.59748401,  0.21887835,\n",
              "        1.51487233,  0.42420585, -0.0967709 ,  2.14971347,  0.5901819 ,\n",
              "        0.5059308 ,  0.62442332, -2.21727122,  0.07743141,  0.28277578,\n",
              "       -1.41414179,  1.40843221, -0.01524613,  2.54100927,  0.05188469,\n",
              "       -0.7521478 ,  0.07962996, -1.15009429,  0.01532774, -1.92511637,\n",
              "       -1.37561023, -1.30423031,  0.42367403, -0.28389485, -0.97166518,\n",
              "       -0.06061037, -1.82075937, -1.07627114, -2.9864282 ,  0.45091273,\n",
              "        0.41697216, -1.09215221,  0.11877897,  1.25143188, -0.49974763,\n",
              "       -1.6725526 , -2.1570375 , -0.20001025, -2.18375449,  2.30115911,\n",
              "        0.13686238,  0.2486508 ,  1.35477506,  0.81912596, -0.78799327,\n",
              "       -2.77449386,  0.09057306,  0.46068467,  1.33486307,  0.55399282,\n",
              "       -0.69784044, -0.90869768, -4.20617083,  3.91538554, -3.12258196,\n",
              "       -0.80098724,  1.20520234,  0.18335362,  0.4682528 ,  0.0993557 ,\n",
              "        0.68941846,  1.84616353, -1.11968866, -0.29862158,  0.57687673,\n",
              "        0.04141472, -0.69864877, -1.54470038,  1.55234992,  0.99717215,\n",
              "       -1.91919117, -0.5928046 , -1.37372972, -1.79230457,  2.97404745,\n",
              "       -1.3114171 , -2.36379977, -0.44801591,  0.83145465,  2.05817205,\n",
              "        0.32770192,  1.2057108 ,  0.28075246, -1.15080368, -1.05453658,\n",
              "        2.21415399, -1.39234436, -3.49305822,  2.204842  ,  1.0616561 ,\n",
              "        2.61937428,  0.34025545,  2.27851312,  0.65013256, -4.51420426,\n",
              "        1.21387204, -0.68840281,  0.13176551,  2.40257829,  2.76787072,\n",
              "       -0.35283464, -0.19073685,  0.03466697, -2.485296  , -0.49098251,\n",
              "       -0.11180494, -1.34034759, -0.19836725,  0.31505086,  0.82678802,\n",
              "        0.27839922, -1.29206491,  2.53503445,  0.09159182, -1.52277164,\n",
              "        1.67974729, -1.10944799, -0.06139156,  0.3169502 ,  0.32798211,\n",
              "        1.36937211, -2.55185725,  2.1448344 ,  1.06672806,  0.5638032 ,\n",
              "       -0.58545578, -0.19763738,  0.04460378, -0.04548242,  2.42037123,\n",
              "        0.23958506,  2.49215903,  1.31672892, -1.75732096, -0.44129863,\n",
              "       -0.87922517, -1.65893845, -0.04476829, -2.26313818, -1.48972705,\n",
              "       -2.04640037,  0.46399595, -0.26630637, -2.05390949, -3.77893412,\n",
              "        2.0922674 , -1.63620042, -3.20730354,  1.90281679,  3.32766201,\n",
              "       -0.74401363,  0.40205681, -0.75428005, -0.19446766,  1.20639036,\n",
              "       -0.95985872,  1.1790349 ,  1.00227673,  1.16192758, -0.40160897,\n",
              "       -0.04718169, -0.85141113, -1.45176721,  0.85446647,  0.86665893,\n",
              "        0.70480499,  0.01713042, -0.23469668, -0.42446989, -3.40098277,\n",
              "        0.92139394, -2.59875455, -2.25198717, -1.76897412, -1.4405646 ,\n",
              "        0.17099364,  1.27461062, -0.21415116,  0.5962249 ,  1.91953927,\n",
              "       -0.6132368 ,  1.4225772 , -1.02550635, -0.67178825,  4.10977118,\n",
              "        0.44792558, -0.38795795, -2.17813694,  0.01603861,  0.50805059,\n",
              "        0.11223909, -0.17917715, -0.66437298, -2.49863088, -1.91525544,\n",
              "        1.02127784,  0.55115861,  0.95207831,  1.36295903, -2.07354331,\n",
              "        0.07573964,  2.64753855, -1.0597031 ,  0.40126488,  1.61396416,\n",
              "        1.40144703,  0.55249709,  1.84749764, -1.15002152, -2.00921217,\n",
              "        0.71820965, -1.61743337, -2.07971691, -1.50477919, -1.81314742,\n",
              "       -1.09326152, -0.77495033, -3.11380345, -1.04984257,  0.84946065,\n",
              "       -0.59543204,  0.41604282,  1.93789186,  0.21326745,  2.14030349])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nice! We got a resulting vector, which is given by the sum of all vectors from the treated text. Now, let's create a function to get the resulting vectors for our entire dataset:"
      ],
      "metadata": {
        "id": "AoQXMAvjhTXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def GetAllResVectors(texts, w2v_model, nlp):\n",
        "  x = len(texts)\n",
        "  y = 300\n",
        "  resulting_vectors = np.zeros((x, y))\n",
        "\n",
        "  for i in range(x):\n",
        "    text_i = texts.iloc[i]\n",
        "    vector = SumCombination(text_i, w2v_model, nlp)\n",
        "    resulting_vectors[i] = vector\n",
        "\n",
        "  return resulting_vectors"
      ],
      "metadata": {
        "id": "G-Bj89uYgcPF"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we do:"
      ],
      "metadata": {
        "id": "JHk1w0vWhxLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = GetAllResVectors(df_train.title_treat, w2v_cbow, nlp)\n",
        "X_test  = GetAllResVectors(df_test.title_treat, w2v_cbow, nlp)"
      ],
      "metadata": {
        "id": "ClOKb907hwiJ"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that, here, we created our vectors using the CBOW model. Later, we will compare our results with the ones using SKIPGRAM. We can also get our target features using:"
      ],
      "metadata": {
        "id": "XG0jHcOSieNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = df_train.category\n",
        "y_test  = df_test.category"
      ],
      "metadata": {
        "id": "IIGe58TQirXV"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dummy classifier\n",
        "\n",
        "Before fitting more complex models, let's fit a dummy classifier, and see how does it behave. This model will serve as a baseline. We can do this with:"
      ],
      "metadata": {
        "id": "O_SL3ZWUils8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dummy = DummyClassifier( )\n",
        "dummy.fit(X_train, y_train)\n",
        "acc = dummy.score(X_test, y_test)*100\n",
        "print(\"Accuracy: {:.2f}%\".format(acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxfRkJTkh0lz",
        "outputId": "3bf6604e-5e24-4cb0-cb29-2a17bb82ee62"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 25.40%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our dummy classifier got a very low accuracy. Let's also get a classification report:"
      ],
      "metadata": {
        "id": "dl5UqN0wi_dp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = dummy.predict(X_test)\n",
        "cr = classification_report(y_test, y_pred)\n",
        "print(cr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMNl0Wtdi-dc",
        "outputId": "2c762dce-2bf7-4ab3-e2c7-c7562c876540"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     colunas       0.00      0.00      0.00      3940\n",
            "   cotidiano       0.00      0.00      0.00      1696\n",
            "     esporte       0.25      1.00      0.41      4657\n",
            "   ilustrada       0.00      0.00      0.00       131\n",
            "     mercado       0.00      0.00      0.00      5861\n",
            "       mundo       0.00      0.00      0.00      2050\n",
            "\n",
            "    accuracy                           0.25     18335\n",
            "   macro avg       0.04      0.17      0.07     18335\n",
            "weighted avg       0.06      0.25      0.10     18335\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems that our dummy classifier identified that our highest frequency category is 'esporte', and it considered that all news were from that category. It showed a 100% recall for this category, but 0.0% for the others. "
      ],
      "metadata": {
        "id": "s6-yXSAijH-I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic regression - Using CBOW\n",
        "\n",
        "Finally, let's fit a logistic regression using the vectors obtained via CBOW. These vectors can be obtained using:"
      ],
      "metadata": {
        "id": "DCqlpvxXjXqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = GetAllResVectors(df_train.title_treat, w2v_cbow, nlp)\n",
        "X_test  = GetAllResVectors(df_test.title_treat, w2v_cbow, nlp)"
      ],
      "metadata": {
        "id": "vJhzTU5TjF1W"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To fit the Logistic Regression model and get its accuracy, we can do:"
      ],
      "metadata": {
        "id": "EGc9bVIqji3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logreg = LogisticRegression(max_iter = 200)\n",
        "logreg.fit(X_train, y_train)\n",
        "acc = logreg.score(X_test, y_test)*100\n",
        "print(\"Accuracy: {:.2f}%\".format(acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4AYLBprjieJ",
        "outputId": "61718c85-07bb-469b-c028-3388c66174f8"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 76.25%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nice! Our model had some issues with convergence, but, still, we got a much higher accuracy this time. If we plot the classification report:"
      ],
      "metadata": {
        "id": "ZiyQibGYj7su"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = logreg.predict(X_test)\n",
        "cr = classification_report(y_test, y_pred)\n",
        "print(cr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ar0kNJ3jpSY",
        "outputId": "fafc5982-538d-40b9-adab-2b3b5867c934"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     colunas       0.70      0.53      0.60      3940\n",
            "   cotidiano       0.63      0.81      0.71      1696\n",
            "     esporte       0.92      0.87      0.90      4657\n",
            "   ilustrada       0.12      0.85      0.22       131\n",
            "     mercado       0.83      0.79      0.81      5861\n",
            "       mundo       0.73      0.84      0.78      2050\n",
            "\n",
            "    accuracy                           0.76     18335\n",
            "   macro avg       0.66      0.78      0.67     18335\n",
            "weighted avg       0.79      0.76      0.77     18335\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the overall recall and precision for most categories is now much higher. "
      ],
      "metadata": {
        "id": "y-21fry8kC50"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic regression - Using SKIPGRAM\n",
        "\n",
        "Now, for comparison purposes, we can try to fit a similar model, but now using the SKIPGRAM model to get the resulting vectors. These vectors can be obtained using:"
      ],
      "metadata": {
        "id": "14KWWrvIkJnT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = GetAllResVectors(df_train.title_treat, w2v_skip, nlp)\n",
        "X_test  = GetAllResVectors(df_test.title_treat, w2v_skip, nlp)"
      ],
      "metadata": {
        "id": "8Tl3OC1QkJnU"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To fit the Logistic Regression model and get its accuracy, we can do:"
      ],
      "metadata": {
        "id": "VVGaQOjwkJnU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logreg = LogisticRegression(max_iter = 200)\n",
        "logreg.fit(X_train, y_train)\n",
        "acc = logreg.score(X_test, y_test)*100\n",
        "print(\"Accuracy: {:.2f}%\".format(acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HK6wz_akJnV",
        "outputId": "b9fe321c-fa89-49eb-ac94-6e1fbcdfad2c"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 77.09%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the SKIPGRAM, we got a slightly higher accuracy. Let's get the classification report:"
      ],
      "metadata": {
        "id": "_BojdsPfkJnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = logreg.predict(X_test)\n",
        "cr = classification_report(y_test, y_pred)\n",
        "print(cr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMsM-ME2kJnV",
        "outputId": "e328e589-f654-4d51-b5c0-820fd1408cfd"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     colunas       0.70      0.53      0.61      3940\n",
            "   cotidiano       0.64      0.80      0.71      1696\n",
            "     esporte       0.93      0.88      0.91      4657\n",
            "   ilustrada       0.14      0.88      0.24       131\n",
            "     mercado       0.83      0.80      0.82      5861\n",
            "       mundo       0.75      0.85      0.80      2050\n",
            "\n",
            "    accuracy                           0.77     18335\n",
            "   macro avg       0.67      0.79      0.68     18335\n",
            "weighted avg       0.80      0.77      0.78     18335\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In general, it seems that the precision and recall are slightly higher (when compared to the CBOW model). In this case, the SKIPGRAM seems like a better method to perform the vectorization of our words. "
      ],
      "metadata": {
        "id": "PLuwW-0ckJnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sELVf53fl9o8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "09_Word2Vec_Training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1D3Gt5Avovmbtm0Yi62hRlmLfWw5mJczx",
      "authorship_tag": "ABX9TyNGwo1ZasW+LACEgNgHdLrp",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}